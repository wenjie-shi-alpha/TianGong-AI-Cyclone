"""Pipeline orchestration for the tropical cyclone environment extractor."""

from __future__ import annotations

from pathlib import Path

from .extractor import TCEnvironmentalSystemsExtractor
from .workflow_utils import (
    combine_initial_tracker_outputs,
    download_s3_public,
    extract_forecast_tag,
    sanitize_filename,
)


# ================= æ–°å¢: æµå¼é¡ºåºå¤„ç†å‡½æ•° =================
def streaming_from_csv(
    csv_path: Path,
    limit: int | None = None,
    search_range: float = 3.0,
    memory: int = 3,
    keep_nc: bool = False,
    initials_csv: Path | None = None,
):
    """é€è¡Œè¯»å–CSV, æ¯ä¸ªNCæ–‡ä»¶æ‰§è¡Œ: ä¸‹è½½ -> è¿½è¸ª -> ç¯å¢ƒåˆ†æ -> (å¯é€‰åˆ é™¤)

    ä¸åŸæ‰¹é‡æ¨¡å¼æœ€å¤§åŒºåˆ«: ä¸é¢„å…ˆä¸‹è½½å…¨éƒ¨; æ¯ä¸ªæ–‡ä»¶å®Œæˆåå³å¯é‡Šæ”¾ç£ç›˜ã€‚
    """
    if not csv_path.exists():
        print(f"âŒ CSVä¸å­˜åœ¨: {csv_path}")
        return
    import pandas as pd, traceback
    from initialTracker import track_file_with_initials as it_track_file_with_initials
    from initialTracker import _load_all_points as it_load_initial_points

    df = pd.read_csv(csv_path)
    required_cols = {"s3_url", "model_prefix", "init_time"}
    if not required_cols.issubset(df.columns):
        print(f"âŒ CSVç¼ºå°‘å¿…è¦åˆ—: {required_cols - set(df.columns)}")
        return
    if limit is not None:
        df = df.head(limit)
    print(f"ğŸ“„ æµå¼å¾…å¤„ç†æ•°é‡: {len(df)} (limit={limit})")

    persist_dir = Path("data/nc_files")
    persist_dir.mkdir(parents=True, exist_ok=True)
    track_dir = Path("track_single")
    track_dir.mkdir(exist_ok=True)
    final_dir = Path("final_single_output")
    final_dir.mkdir(exist_ok=True)

    processed = 0
    skipped = 0
    for idx, row in df.iterrows():
        s3_url = row["s3_url"]
        model_prefix = row["model_prefix"]
        init_time = row["init_time"]
        fname = Path(s3_url).name
        forecast_tag = extract_forecast_tag(fname)
        safe_prefix = sanitize_filename(model_prefix)
        safe_init = sanitize_filename(init_time.replace(":", "").replace("-", ""))
        track_csv = track_dir / f"tracks_{safe_prefix}_{safe_init}_{forecast_tag}.csv"
        nc_local = persist_dir / fname

        print(f"\n[{idx+1}/{len(df)}] â–¶ï¸ å¤„ç†: {fname}")

        existing_json = list(final_dir.glob(f"{Path(fname).stem}_TC_Analysis_*.json"))
        if existing_json:
            non_empty = [p for p in existing_json if p.stat().st_size > 10]
            if non_empty:
                print(f"â­ï¸  å·²å­˜åœ¨æœ€ç»ˆJSON({len(non_empty)}) -> è·³è¿‡")
                skipped += 1
                continue

        if not nc_local.exists():
            try:
                print(f"â¬‡ï¸  ä¸‹è½½NC: {s3_url}")
                download_s3_public(s3_url, nc_local)
            except Exception as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥, è·³è¿‡: {e}")
                skipped += 1
                continue
        else:
            print("ğŸ“¦ å·²å­˜åœ¨NCæ–‡ä»¶, å¤ç”¨")

        if not track_csv.exists():
            try:
                print("ğŸ§­ ä½¿ç”¨ initialTracker æ‰§è¡Œè¿½è¸ª...")
                initials_path = initials_csv or Path("input/western_pacific_typhoons_superfast.csv")
                initials_df = it_load_initial_points(initials_path)
                per_storm_csvs = it_track_file_with_initials(Path(nc_local), initials_df, track_dir)
                if not per_storm_csvs:
                    print("âš ï¸ æ— æœ‰æ•ˆè½¨è¿¹ -> è·³è¿‡ç¯å¢ƒåˆ†æ")
                    if not keep_nc:
                        try:
                            nc_local.unlink()
                            print("ğŸ§¹ å·²åˆ é™¤NC (æ— è½¨è¿¹)")
                        except Exception:
                            pass
                    skipped += 1
                    continue

                combined = combine_initial_tracker_outputs(per_storm_csvs, nc_local)
                if combined is None or combined.empty:
                    print("âš ï¸ æ— æ³•åˆå¹¶è½¨è¿¹è¾“å‡º -> è·³è¿‡ç¯å¢ƒåˆ†æ")
                    if not keep_nc:
                        try:
                            nc_local.unlink()
                            print("ğŸ§¹ å·²åˆ é™¤NC (æ— è½¨è¿¹)")
                        except Exception:
                            pass
                    skipped += 1
                    continue
                combined.to_csv(track_csv, index=False)
                print(
                    f"ğŸ’¾ åˆå¹¶ä¿å­˜è½¨è¿¹: {track_csv.name} (å« {combined['particle'].nunique()} æ¡è·¯å¾„)"
                )
            except Exception as e:
                print(f"âŒ è¿½è¸ªå¤±è´¥: {e}")
                traceback.print_exc()
                if not keep_nc:
                    try:
                        nc_local.unlink()
                        print("ğŸ§¹ å·²åˆ é™¤NC (è¿½è¸ªå¤±è´¥)")
                    except Exception:
                        pass
                skipped += 1
                continue
        else:
            print("ğŸ—ºï¸  å·²å­˜åœ¨è½¨è¿¹CSV, ç›´æ¥ç¯å¢ƒåˆ†æ")

        try:
            extractor = TCEnvironmentalSystemsExtractor(str(nc_local), str(track_csv))
            extractor.analyze_and_export_as_json("final_single_output")
            processed += 1
        except Exception as e:
            print(f"âŒ ç¯å¢ƒåˆ†æå¤±è´¥: {e}")
        finally:
            if not keep_nc:
                try:
                    nc_local.unlink()
                    print("ğŸ§¹ å·²åˆ é™¤NCæ–‡ä»¶")
                except Exception as ee:
                    print(f"âš ï¸ åˆ é™¤NCå¤±è´¥: {ee}")

    print("\nğŸ“Š æµå¼å¤„ç†ç»“æœ:")
    print(f"  âœ… å®Œæˆ: {processed}")
    print(f"  â­ï¸ è·³è¿‡: {skipped}")
    print(f"  ğŸ“ è¾“å‡ºç›®å½•: final_single_output")


def process_nc_files(target_nc_files, args):
    """å¤„ç†å·²å‡†å¤‡å¥½çš„ NC æ–‡ä»¶åˆ—è¡¨ï¼Œä¿æŒ legacy è¡Œä¸ºä¸å˜ã€‚"""
    import pandas as pd

    final_output_dir = Path("final_single_output")
    final_output_dir.mkdir(exist_ok=True)

    processed = 0
    skipped = 0
    for idx, nc_file in enumerate(target_nc_files, start=1):
        import re

        nc_stem = nc_file.stem
        print(f"\n[{idx}/{len(target_nc_files)}] â–¶ï¸ å¤„ç† NC: {nc_file.name}")
        existing = list(final_output_dir.glob(f"{nc_stem}_TC_Analysis_*.json"))
        non_empty = [p for p in existing if p.stat().st_size > 10]
        if non_empty:
            print(f"â­ï¸  å·²å­˜åœ¨åˆ†æç»“æœ ({len(non_empty)}) -> è·³è¿‡ {nc_stem}")
            skipped += 1
            continue

        track_file = None
        if args.tracks:
            t = Path(args.tracks)
            if t.exists():
                track_file = t
        if track_file is None:
            tdir = Path("track_single")
            if tdir.exists():
                forecast_tag_match = re.search(r"(f\d{3}_f\d{3}_\d{2})", nc_stem)
                potential = []
                if forecast_tag_match:
                    tag = forecast_tag_match.group(1)
                    potential = list(tdir.glob(f"tracks_*_{tag}.csv"))
                tracks_all = sorted(tdir.glob("tracks_*.csv"))
                if potential:
                    track_file = potential[0]
                elif tracks_all:
                    track_file = tracks_all[0]
                    print(f"âš ï¸ æœªç²¾ç¡®åŒ¹é… forecast_tag, ä½¿ç”¨ {track_file.name}")
        if track_file is None:
            if args.auto:
                from initialTracker import track_file_with_initials as it_track_file_with_initials
                from initialTracker import _load_all_points as it_load_initial_points

                print("ğŸ”„ ä½¿ç”¨ initialTracker è‡ªåŠ¨è¿½è¸ªå½“å‰NCä»¥ç”Ÿæˆè½¨è¿¹...")
                try:
                    initials_path = (
                        Path(args.initials)
                        if args.initials
                        else Path("input/western_pacific_typhoons_superfast.csv")
                    )
                    initials_df = it_load_initial_points(initials_path)
                    out_dir = Path("track_single")
                    out_dir.mkdir(exist_ok=True)
                    per_storm = it_track_file_with_initials(Path(nc_file), initials_df, out_dir)
                    if not per_storm:
                        print("âš ï¸ æ— è½¨è¿¹ -> è·³è¿‡è¯¥NC")
                        skipped += 1
                        continue
                    combined = combine_initial_tracker_outputs(per_storm, nc_file)
                    if combined is None or combined.empty:
                        print("âš ï¸ è‡ªåŠ¨è¿½è¸ªæ— æœ‰æ•ˆè½¨è¿¹ -> è·³è¿‡è¯¥NC")
                        skipped += 1
                        continue
                    first_time = (
                        combined.iloc[0]["time"] if "time" in combined.columns else None
                    )
                    ts0 = (
                        pd.to_datetime(first_time).strftime("%Y%m%d%H")
                        if pd.notnull(first_time)
                        else "T000"
                    )
                    track_file = out_dir / f"tracks_auto_{nc_stem}_{ts0}.csv"
                    combined.to_csv(track_file, index=False)
                    print(
                        f"ğŸ’¾ è‡ªåŠ¨è½¨è¿¹æ–‡ä»¶: {track_file.name} (å« {combined['particle'].nunique()} æ¡è·¯å¾„)"
                    )
                except Exception as e:
                    print(f"âŒ è‡ªåŠ¨è¿½è¸ªå¤±è´¥: {e}")
                    skipped += 1
                    continue
            else:
                print("âš ï¸ æœªæ‰¾åˆ°å¯¹åº”è½¨è¿¹ä¸”æœªå¯ç”¨ --auto, è·³è¿‡")
                skipped += 1
                continue

        print(f"âœ… ä½¿ç”¨è½¨è¿¹æ–‡ä»¶: {track_file}")
        try:
            extractor = TCEnvironmentalSystemsExtractor(str(nc_file), str(track_file))
            extractor.analyze_and_export_as_json("final_single_output")
            processed += 1
        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥ {nc_file.name}: {e}")
            continue

        if not (args.no_clean or args.keep_nc):
            try:
                nc_file.unlink()
                print(f"ğŸ§¹ å·²åˆ é™¤ NC: {nc_file.name}")
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤NCå¤±è´¥: {e}")
        else:
            print("â„¹ï¸ æŒ‰å‚æ•°ä¿ç•™NCæ–‡ä»¶")

    print("\nğŸ‰ å¤šæ–‡ä»¶ç¯å¢ƒåˆ†æå®Œæˆ. ç»Ÿè®¡:")
    print(f"  âœ… å·²åˆ†æ: {processed}")
    print(f"  â­ï¸ è·³è¿‡(å·²æœ‰ç»“æœ/æ— è½¨è¿¹): {skipped}")
    print(f"  ğŸ“¦ æ€»è®¡éå†: {len(target_nc_files)}")
    print("ç»“æœç›®å½•: final_single_output")

    return processed, skipped
