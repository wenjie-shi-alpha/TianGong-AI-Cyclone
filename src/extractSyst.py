#!/usr/bin/env python3
"""
çƒ­å¸¦æ°”æ—‹ç¯å¢ƒåœºå½±å“ç³»ç»Ÿæå–å™¨ï¼ˆä¸“å®¶è§£è¯‘ç‰ˆï¼‰
åŸºäºå·²è¿½è¸ªçš„çƒ­å¸¦æ°”æ—‹è·¯å¾„ï¼Œæå–å¹¶è¯¦ç»†è§£è¯‘å½±å“å…¶ç§»åŠ¨å’Œå¼ºåº¦çš„å…³é”®å¤©æ°”ç³»ç»Ÿï¼Œ
è¾“å‡ºåŒ…å«æ°”è±¡æè¿°ã€å®šæ€§åˆ†çº§å’Œå½¢çŠ¶åæ ‡çš„ç»“æ„åŒ–JSONæ–‡ä»¶ã€‚
"""

import xarray as xr
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import json
import math
import warnings
import re

# å¼•å…¥æ–°çš„åº“ç”¨äºå›¾åƒå¤„ç†å’Œç­‰å€¼çº¿æå–
try:
    from scipy.ndimage import label, center_of_mass, find_objects, binary_erosion, binary_dilation
    from skimage.measure import find_contours, regionprops, approximate_polygon
    from skimage.morphology import convex_hull_image
    from scipy.spatial.distance import pdist
    from scipy.spatial import ConvexHull
except ImportError:
    print("é”™è¯¯ï¼šéœ€è¦scipyå’Œscikit-imageåº“ã€‚è¯·è¿è¡Œ 'pip install scipy scikit-image' è¿›è¡Œå®‰è£…ã€‚")
    exit()

warnings.filterwarnings("ignore")


class WeatherSystemShapeAnalyzer:
    """
    æ°”è±¡ç³»ç»Ÿå½¢çŠ¶åˆ†æå™¨
    ä¸“é—¨ç”¨äºåˆ†ææ°”è±¡ç³»ç»Ÿçš„å‡ ä½•å½¢çŠ¶ç‰¹å¾
    """

    def __init__(self, lat_grid, lon_grid):
        self.lat = lat_grid
        self.lon = lon_grid
        self.lat_spacing = np.abs(np.diff(lat_grid).mean())
        self.lon_spacing = np.abs(np.diff(lon_grid).mean())

    def analyze_system_shape(
        self, data_field, threshold, system_type="high", center_lat=None, center_lon=None
    ):
        """
        å…¨é¢åˆ†ææ°”è±¡ç³»ç»Ÿçš„å½¢çŠ¶ç‰¹å¾

        Parameters:
        -----------
        data_field : numpy.ndarray
            äºŒç»´æ°”è±¡æ•°æ®åœº
        threshold : float
            ç”¨äºå®šä¹‰ç³»ç»Ÿè¾¹ç•Œçš„é˜ˆå€¼
        system_type : str
            ç³»ç»Ÿç±»å‹ ('high' æˆ– 'low')
        center_lat, center_lon : float
            ç³»ç»Ÿä¸­å¿ƒä½ç½®ï¼ˆç”¨äºè®¡ç®—ç›¸å¯¹ç‰¹å¾ï¼‰

        Returns:
        --------
        dict : åŒ…å«è¯¦ç»†å½¢çŠ¶ç‰¹å¾çš„å­—å…¸
        """
        try:
            # 1. åˆ›å»ºäºŒå€¼æ©è†œ
            if system_type == "high":
                mask = data_field >= threshold
            else:
                mask = data_field <= threshold

            if not np.any(mask):
                return None

            # 2. è¿é€šåŒºåŸŸåˆ†æ
            labeled_mask, num_features = label(mask)
            if num_features == 0:
                return None

            # 3. é€‰æ‹©ä¸»è¦ç³»ç»Ÿï¼ˆæœ€å¤§æˆ–ç¦»ä¸­å¿ƒæœ€è¿‘çš„ï¼‰
            main_region = self._select_main_system(
                labeled_mask, num_features, center_lat, center_lon
            )
            if main_region is None:
                return None

            # 4. è®¡ç®—åŸºç¡€å‡ ä½•ç‰¹å¾
            basic_features = self._calculate_basic_features(
                main_region, data_field, threshold, system_type
            )

            # 5. è®¡ç®—å½¢çŠ¶å¤æ‚åº¦ç‰¹å¾
            complexity_features = self._calculate_complexity_features(main_region)

            # 6. è®¡ç®—æ–¹å‘æ€§ç‰¹å¾
            orientation_features = self._calculate_orientation_features(main_region)

            # 7. æå–ç­‰å€¼çº¿ç‰¹å¾
            contour_features = self._extract_contour_features(data_field, threshold, system_type)

            # 8. è®¡ç®—å¤šå°ºåº¦ç‰¹å¾
            multiscale_features = self._calculate_multiscale_features(
                data_field, threshold, system_type
            )

            return {
                "basic_geometry": basic_features,
                "shape_complexity": complexity_features,
                "orientation": orientation_features,
                "contour_analysis": contour_features,
                "multiscale_features": multiscale_features,
            }

        except Exception as e:
            print(f"å½¢çŠ¶åˆ†æå¤±è´¥: {e}")
            return None

    def _select_main_system(self, labeled_mask, num_features, center_lat, center_lon):
        """é€‰æ‹©ä¸»è¦çš„æ°”è±¡ç³»ç»ŸåŒºåŸŸ"""
        if center_lat is None or center_lon is None:
            # é€‰æ‹©æœ€å¤§çš„è¿é€šåŒºåŸŸ (ä½¿ç”¨ bincount ç­‰ä»·åŠ é€Ÿ)
            flat_labels = labeled_mask.ravel()
            counts = np.bincount(flat_labels)[1: num_features + 1]
            if counts.size == 0:
                return None
            main_label = int(np.argmax(counts) + 1)
        else:
            # é€‰æ‹©ç¦»æŒ‡å®šä¸­å¿ƒæœ€è¿‘çš„åŒºåŸŸ
            center_lat_idx = np.abs(self.lat - center_lat).argmin()
            center_lon_idx = np.abs(self.lon - center_lon).argmin()

            min_dist = float("inf")
            main_label = 1

            for i in range(1, num_features + 1):
                region_mask = labeled_mask == i
                com_y, com_x = center_of_mass(region_mask)
                dist = np.sqrt((com_y - center_lat_idx) ** 2 + (com_x - center_lon_idx) ** 2)
                if dist < min_dist:
                    min_dist = dist
                    main_label = i

        return labeled_mask == main_label

    def _calculate_basic_features(self, region_mask, data_field, threshold, system_type):
        """è®¡ç®—åŸºç¡€å‡ ä½•ç‰¹å¾"""
        # ä½¿ç”¨regionpropsè¿›è¡Œé«˜çº§å½¢çŠ¶åˆ†æ
        props = regionprops(region_mask.astype(int), intensity_image=data_field)[0]

        # è®¡ç®—å®é™…çš„åœ°ç†é¢ç§¯ï¼ˆkmÂ²ï¼‰
        area_pixels = props.area
        area_km2 = (
            area_pixels
            * (self.lat_spacing * 111)
            * (self.lon_spacing * 111 * np.cos(np.deg2rad(np.mean(self.lat))))
        )

        # è®¡ç®—å‘¨é•¿ï¼ˆkmï¼‰
        perimeter_pixels = props.perimeter
        perimeter_km = perimeter_pixels * np.sqrt(
            (self.lat_spacing * 111) ** 2 + (self.lon_spacing * 111) ** 2
        )

        # è®¡ç®—ç´§å‡‘åº¦å’Œå½¢çŠ¶æŒ‡æ•°
        compactness = 4 * np.pi * area_km2 / (perimeter_km**2) if perimeter_km > 0 else 0
        shape_index = perimeter_km / (2 * np.sqrt(np.pi * area_km2)) if area_km2 > 0 else 0

        # è®¡ç®—é•¿å®½æ¯”å’Œåå¿ƒç‡
        major_axis_length = props.major_axis_length * self.lat_spacing * 111  # km
        minor_axis_length = props.minor_axis_length * self.lat_spacing * 111  # km
        aspect_ratio = major_axis_length / minor_axis_length if minor_axis_length > 0 else 1
        eccentricity = props.eccentricity

        # è®¡ç®—å¼ºåº¦ç»Ÿè®¡
        intensity_values = data_field[region_mask]
        if system_type == "high":
            max_intensity = np.max(intensity_values)
            intensity_range = max_intensity - threshold
        else:
            min_intensity = np.min(intensity_values)
            intensity_range = threshold - min_intensity

        return {
            "area_km2": round(area_km2, 1),
            "perimeter_km": round(perimeter_km, 1),
            "compactness": round(compactness, 3),
            "shape_index": round(shape_index, 3),
            "aspect_ratio": round(aspect_ratio, 2),
            "eccentricity": round(eccentricity, 3),
            "major_axis_km": round(major_axis_length, 1),
            "minor_axis_km": round(minor_axis_length, 1),
            "intensity_range": round(intensity_range, 1),
            "description": self._describe_basic_shape(compactness, aspect_ratio, eccentricity),
        }

    def _calculate_complexity_features(self, region_mask):
        """è®¡ç®—å½¢çŠ¶å¤æ‚åº¦ç‰¹å¾"""
        # è®¡ç®—å‡¸åŒ…
        convex_hull = convex_hull_image(region_mask)
        convex_area = np.sum(convex_hull)
        actual_area = np.sum(region_mask)

        # å‡¸æ€§ï¼ˆsolidityï¼‰
        solidity = actual_area / convex_area if convex_area > 0 else 0

        # è®¡ç®—è¾¹ç•Œç²—ç³™åº¦
        contours = find_contours(region_mask.astype(float), 0.5)
        if contours:
            main_contour = max(contours, key=len)
            # ä½¿ç”¨å¤šè¾¹å½¢è¿‘ä¼¼æ¥è¯„ä¼°è¾¹ç•Œå¤æ‚åº¦
            epsilon = 0.02 * len(main_contour)
            approx_contour = approximate_polygon(main_contour, tolerance=epsilon)
            boundary_complexity = (
                len(main_contour) / len(approx_contour) if len(approx_contour) > 0 else 1
            )
        else:
            boundary_complexity = 1

        # åˆ†å½¢ç»´æ•°è¿‘ä¼¼
        fractal_dimension = self._estimate_fractal_dimension(region_mask)

        return {
            "solidity": round(solidity, 3),
            "boundary_complexity": round(boundary_complexity, 2),
            "fractal_dimension": round(fractal_dimension, 3),
            "description": self._describe_complexity(solidity, boundary_complexity),
        }

    def _calculate_orientation_features(self, region_mask):
        """è®¡ç®—æ–¹å‘æ€§ç‰¹å¾"""
        props = regionprops(region_mask.astype(int))[0]

        # ä¸»è½´æ–¹å‘è§’ï¼ˆå¼§åº¦è½¬åº¦ï¼‰
        orientation_rad = props.orientation
        orientation_deg = np.degrees(orientation_rad)

        # æ ‡å‡†åŒ–åˆ°0-180åº¦
        if orientation_deg < 0:
            orientation_deg += 180

        # ç¡®å®šä¸»è¦å»¶ä¼¸æ–¹å‘
        if 0 <= orientation_deg < 22.5 or 157.5 <= orientation_deg <= 180:
            direction_desc = "å—åŒ—å‘å»¶ä¼¸"
        elif 22.5 <= orientation_deg < 67.5:
            direction_desc = "ä¸œåŒ—-è¥¿å—å‘å»¶ä¼¸"
        elif 67.5 <= orientation_deg < 112.5:
            direction_desc = "ä¸œè¥¿å‘å»¶ä¼¸"
        else:
            direction_desc = "è¥¿åŒ—-ä¸œå—å‘å»¶ä¼¸"

        return {
            "orientation_deg": round(orientation_deg, 1),
            "direction_type": direction_desc,
            "description": f"ç³»ç»Ÿä¸»è½´å‘ˆ{direction_desc}ï¼Œæ–¹å‘è§’ä¸º{orientation_deg:.1f}Â°",
        }

    def _extract_contour_features(self, data_field, threshold, system_type):
        """æå–ç­‰å€¼çº¿ç‰¹å¾"""
        try:
            contours = find_contours(data_field, threshold)
            if not contours:
                return None

            # é€‰æ‹©æœ€é•¿çš„ç­‰å€¼çº¿
            main_contour = max(contours, key=len)

            # è½¬æ¢ä¸ºåœ°ç†åæ ‡
            contour_lats = self.lat[main_contour[:, 0].astype(int)]
            contour_lons = self.lon[main_contour[:, 1].astype(int)]

            # è®¡ç®—ç­‰å€¼çº¿é•¿åº¦
            contour_length_km = 0
            for i in range(1, len(contour_lats)):
                dist = self._haversine_distance(
                    contour_lats[i - 1], contour_lons[i - 1], contour_lats[i], contour_lons[i]
                )
                contour_length_km += dist

            # é™é‡‡æ ·ç­‰å€¼çº¿ç‚¹ä»¥å‡å°‘æ•°æ®é‡
            step = max(1, len(main_contour) // 50)
            simplified_contour = [
                [round(lon, 2), round(lat, 2)]
                for lat, lon in zip(contour_lats[::step], contour_lons[::step])
            ]

            # æå–å¤šè¾¹å½¢åæ ‡ç‰¹å¾
            polygon_features = self._extract_polygon_coordinates(main_contour, data_field.shape)

            return {
                "contour_length_km": round(contour_length_km, 1),
                "contour_points": len(main_contour),
                "simplified_coordinates": simplified_contour,
                "polygon_features": polygon_features,
                "description": f"ä¸»ç­‰å€¼çº¿é•¿åº¦{contour_length_km:.0f}kmï¼ŒåŒ…å«{len(main_contour)}ä¸ªæ•°æ®ç‚¹",
            }
        except Exception:
            return None

    def _extract_polygon_coordinates(self, contour, shape):
        """æå–å¤šè¾¹å½¢å…³é”®åæ ‡ç‚¹"""
        try:
            # ä½¿ç”¨å¤šè¾¹å½¢è¿‘ä¼¼æ¥è·å–å…³é”®è§’ç‚¹
            epsilon = 0.02 * len(contour)
            approx_polygon = approximate_polygon(contour, tolerance=epsilon)

            # è½¬æ¢ä¸ºåœ°ç†åæ ‡
            polygon_coords = []
            for point in approx_polygon:
                lat_idx = int(np.clip(point[0], 0, len(self.lat) - 1))
                lon_idx = int(np.clip(point[1], 0, len(self.lon) - 1))
                polygon_coords.append([round(self.lon[lon_idx], 2), round(self.lat[lat_idx], 2)])

            # è®¡ç®—è¾¹ç•Œæ¡†
            if len(polygon_coords) > 0:
                lons = [coord[0] for coord in polygon_coords]
                lats = [coord[1] for coord in polygon_coords]
                bbox = [
                    round(min(lons), 2),
                    round(min(lats), 2),
                    round(max(lons), 2),
                    round(max(lats), 2),
                ]  # [west, south, east, north]

                # è®¡ç®—ä¸­å¿ƒç‚¹
                center = [round(np.mean(lons), 2), round(np.mean(lats), 2)]

                # æå–å…³é”®æ–¹å‘ç‚¹
                cardinal_points = {
                    "N": [lons[lats.index(max(lats))], max(lats)],
                    "S": [lons[lats.index(min(lats))], min(lats)],
                    "E": [max(lons), lats[lons.index(max(lons))]],
                    "W": [min(lons), lats[lons.index(min(lons))]],
                }

                return {
                    "polygon": polygon_coords,
                    "vertices": len(polygon_coords),
                    "bbox": bbox,
                    "center": center,
                    "cardinal_points": cardinal_points,
                    "span": [
                        round(bbox[2] - bbox[0], 2),
                        round(bbox[3] - bbox[1], 2),
                    ],  # [lon_span, lat_span]
                }

            return None
        except Exception as e:
            return None

    def _calculate_multiscale_features(self, data_field, threshold, system_type):
        """è®¡ç®—å¤šå°ºåº¦ç‰¹å¾"""
        features = {}

        # å®šä¹‰å¤šä¸ªé˜ˆå€¼æ°´å¹³
        if system_type == "high":
            thresholds = [threshold, threshold + 20, threshold + 40]
            threshold_names = ["å¤–è¾¹ç•Œ", "ä¸­ç­‰å¼ºåº¦", "å¼ºä¸­å¿ƒ"]
        else:
            thresholds = [threshold, threshold - 20, threshold - 40]
            threshold_names = ["å¤–è¾¹ç•Œ", "ä¸­ç­‰å¼ºåº¦", "å¼ºä¸­å¿ƒ"]

        for i, (thresh, name) in enumerate(zip(thresholds, threshold_names)):
            if system_type == "high":
                mask = data_field >= thresh
            else:
                mask = data_field <= thresh

            if np.any(mask):
                area_pixels = np.sum(mask)
                area_km2 = (
                    area_pixels
                    * (self.lat_spacing * 111)
                    * (self.lon_spacing * 111 * np.cos(np.deg2rad(np.mean(self.lat))))
                )
                features[f"area_{name}_km2"] = round(area_km2, 1)
            else:
                features[f"area_{name}_km2"] = 0

        # è®¡ç®—åµŒå¥—æ¯”ä¾‹
        if features.get("area_å¤–è¾¹ç•Œ_km2", 0) > 0:
            features["core_ratio"] = round(
                features.get("area_å¼ºä¸­å¿ƒ_km2", 0) / features["area_å¤–è¾¹ç•Œ_km2"], 3
            )
            features["middle_ratio"] = round(
                features.get("area_ä¸­ç­‰å¼ºåº¦_km2", 0) / features["area_å¤–è¾¹ç•Œ_km2"], 3
            )

        return features

    def _describe_basic_shape(self, compactness, aspect_ratio, eccentricity):
        """æè¿°åŸºæœ¬å½¢çŠ¶ç‰¹å¾"""
        if compactness > 0.7:
            shape_desc = "è¿‘åœ†å½¢"
        elif compactness > 0.4:
            shape_desc = "è¾ƒè§„åˆ™"
        else:
            shape_desc = "ä¸è§„åˆ™"

        if aspect_ratio > 3:
            elongation_desc = "é«˜åº¦æ‹‰é•¿"
        elif aspect_ratio > 2:
            elongation_desc = "æ˜æ˜¾æ‹‰é•¿"
        elif aspect_ratio > 1.5:
            elongation_desc = "ç•¥å¾®æ‹‰é•¿"
        else:
            elongation_desc = "è¾ƒä¸ºåœ†æ¶¦"

        return f"{shape_desc}çš„{elongation_desc}ç³»ç»Ÿ"

    def _describe_complexity(self, solidity, boundary_complexity):
        """æè¿°å¤æ‚åº¦ç‰¹å¾"""
        if solidity > 0.9:
            complexity_desc = "è¾¹ç•Œå¹³æ»‘"
        elif solidity > 0.7:
            complexity_desc = "è¾¹ç•Œè¾ƒè§„åˆ™"
        else:
            complexity_desc = "è¾¹ç•Œå¤æ‚"

        if boundary_complexity > 2:
            detail_desc = "å…·æœ‰ç²¾ç»†ç»“æ„"
        elif boundary_complexity > 1.5:
            detail_desc = "å…·æœ‰ä¸€å®šç»†èŠ‚"
        else:
            detail_desc = "ç»“æ„ç›¸å¯¹ç®€å•"

        return f"{complexity_desc}ï¼Œ{detail_desc}"

    def _estimate_fractal_dimension(self, region_mask):
        """ä¼°ç®—åˆ†å½¢ç»´æ•°ï¼ˆç®€åŒ–æ–¹æ³•ï¼‰"""
        try:
            # ä½¿ç”¨ç›’è®¡æ•°æ³•çš„ç®€åŒ–ç‰ˆæœ¬
            sizes = [2, 4, 8, 16]
            counts = []

            for size in sizes:
                # å°†å›¾åƒåˆ†å‰²æˆä¸åŒå¤§å°çš„ç›’å­
                h, w = region_mask.shape
                count = 0
                for i in range(0, h, size):
                    for j in range(0, w, size):
                        box = region_mask[i : min(i + size, h), j : min(j + size, w)]
                        if np.any(box):
                            count += 1
                counts.append(count)

            # è®¡ç®—åˆ†å½¢ç»´æ•°
            if len(counts) > 1 and all(c > 0 for c in counts):
                coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)
                fractal_dim = -coeffs[0]
                return max(1.0, min(2.0, fractal_dim))  # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
            else:
                return 1.5  # é»˜è®¤å€¼
        except:
            return 1.5

    def _haversine_distance(self, lat1, lon1, lat2, lon2):
        """è®¡ç®—ä¸¤ç‚¹é—´çš„çƒé¢è·ç¦»ï¼ˆkmï¼‰"""
        R = 6371.0  # åœ°çƒåŠå¾„
        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2
        c = 2 * np.arcsin(np.sqrt(a))
        return R * c


class TCEnvironmentalSystemsExtractor:
    """
    çƒ­å¸¦æ°”æ—‹ç¯å¢ƒåœºå½±å“ç³»ç»Ÿæå–å™¨
    """

    def __init__(self, forecast_data_path, tc_tracks_path):
        # ... (åˆå§‹åŒ–ä»£ç ä¸ä¸Šä¸€ç‰ˆç›¸åŒ) ...
        self.ds = xr.open_dataset(forecast_data_path)
        # ä¿å­˜åŸå§‹NCæ–‡ä»¶å(å«/ä¸å«æ‰©å±•)ä¾›è¾“å‡ºå‘½åä½¿ç”¨
        try:
            p = Path(forecast_data_path)
            self.nc_filename = p.name
            self.nc_stem = p.stem
        except Exception:
            self.nc_filename = "data"
            self.nc_stem = "data"
        self.lat = self.ds.latitude.values if "latitude" in self.ds.coords else self.ds.lat.values
        self.lon = self.ds.longitude.values if "longitude" in self.ds.coords else self.ds.lon.values
        self.lon_180 = np.where(self.lon > 180, self.lon - 360, self.lon)
        self.lat_spacing = np.abs(np.diff(self.lat).mean())
        self.lon_spacing = np.abs(np.diff(self.lon).mean())

        # é¢„è®¡ç®— cos(lat) åŠå…¶å®‰å…¨ç‰ˆæœ¬ï¼ˆé¿å…æåŒºé™¤é›¶æ”¾å¤§ï¼‰ï¼›ä¸æ”¹å˜æ•°å€¼ç­–ç•¥ï¼Œä»…æå‰è®¡ç®—
        self._coslat = np.cos(np.deg2rad(self.lat))
        self._coslat_safe = np.where(np.abs(self._coslat) < 1e-6, np.nan, self._coslat)

        # æ¢¯åº¦ç¼“å­˜ï¼šå­˜å‚¨ (id(array) -> (grad_y_raw, grad_x_raw))ï¼Œä¿æŒä¸ np.gradient(axis=0/1) å®Œå…¨ä¸€è‡´
        self._grad_cache: dict[int, tuple[np.ndarray, np.ndarray]] = {}

        def _raw_gradients(arr: np.ndarray):
            key = id(arr)
            if key in self._grad_cache:
                return self._grad_cache[key]
            gy = np.gradient(arr, axis=0)
            gx = np.gradient(arr, axis=1)
            self._grad_cache[key] = (gy, gx)
            return gy, gx
        self._raw_gradients = _raw_gradients  # ç»‘å®šå®ä¾‹

        # ç»çº¬åº¦ç´¢å¼•è¾…åŠ©ï¼šåŠŸèƒ½ç­‰ä»·äºåŸå¤šæ¬¡ argmin è°ƒç”¨
        def _loc_idx(lat_val: float, lon_val: float):
            return (np.abs(self.lat - lat_val).argmin(), np.abs(self.lon - lon_val).argmin())
        self._loc_idx = _loc_idx

        # åˆå§‹åŒ–å½¢çŠ¶åˆ†æå™¨
        self.shape_analyzer = WeatherSystemShapeAnalyzer(self.lat, self.lon)

        self.tc_tracks = pd.read_csv(tc_tracks_path)
        self.tc_tracks["time"] = pd.to_datetime(self.tc_tracks["time"])

        print(f"ğŸ“Š åŠ è½½{len(self.tc_tracks)}ä¸ªçƒ­å¸¦æ°”æ—‹è·¯å¾„ç‚¹")
        print(
            f"ğŸŒ åŒºåŸŸèŒƒå›´: {self.lat.min():.1f}Â°-{self.lat.max():.1f}Â°N, {self.lon.min():.1f}Â°-{self.lon.max():.1f}Â°E"
        )
        print(f"ğŸ” å¢å¼ºå½¢çŠ¶åˆ†æåŠŸèƒ½å·²å¯ç”¨")

    # --- æ ¸å¿ƒç³»ç»Ÿæå–å‡½æ•° (æ·±åº¦é‡æ„) ---

    def extract_steering_system(self, time_idx, tc_lat, tc_lon):
        """
        [æ·±åº¦é‡æ„] æå–å¹¶è§£è¯‘å¼•å¯¼æ°”æµå’Œå‰¯çƒ­å¸¦é«˜å‹ç³»ç»Ÿã€‚
        """
        try:
            z500 = self._get_data_at_level("z", 500, time_idx)
            if z500 is None:
                return None

            # 1. è¯†åˆ«å‰¯é«˜ç³»ç»Ÿ
            subtropical_high_obj = self._identify_pressure_system(
                z500, tc_lat, tc_lon, "high", 5880
            )
            if not subtropical_high_obj:
                return None

            # 2. å¢å¼ºå½¢çŠ¶åˆ†æ
            enhanced_shape = self._get_enhanced_shape_info(z500, 5880, "high", tc_lat, tc_lon)

            # 3. è®¡ç®—å¼•å¯¼æ°”æµ
            steering_speed, steering_direction, u_steering, v_steering = (
                self._calculate_steering_flow(z500, tc_lat, tc_lon)
            )

            # 4. ä¸°å¯ŒåŒ–æè¿°å’Œå±æ€§
            # 4.1 å¼ºåº¦å®šæ€§åˆ†çº§
            intensity_val = subtropical_high_obj["intensity"]["value"]
            if intensity_val > 5900:
                level = "å¼º"
            elif intensity_val > 5880:
                level = "ä¸­ç­‰"
            else:
                level = "å¼±"
            subtropical_high_obj["intensity"]["level"] = level

            # 4.2 æ›´æ–°å½¢çŠ¶ä¿¡æ¯
            if enhanced_shape:
                subtropical_high_obj["shape"].update(
                    {
                        "detailed_analysis": enhanced_shape["detailed_analysis"],
                        "area_km2": enhanced_shape["area_km2"],
                        "shape_type": enhanced_shape["shape_type"],
                        "orientation": enhanced_shape["orientation"],
                        "complexity": enhanced_shape["complexity"],
                    }
                )

                # æ·»åŠ åæ ‡ä¿¡æ¯
                if "coordinate_info" in enhanced_shape:
                    subtropical_high_obj["shape"]["coordinate_details"] = enhanced_shape[
                        "coordinate_info"
                    ]

            # 4.3 æå–å…³é”®åæ ‡ç‚¹
            system_coords = self._get_system_coordinates(z500, 5880, "high", max_points=15)
            if system_coords:
                subtropical_high_obj["shape"]["coordinates"] = system_coords

            # 4.4 ä¼ ç»Ÿç­‰å€¼çº¿åæ ‡ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
            contour_coords = self._get_contour_coords(z500, 5880, tc_lon)
            if contour_coords:
                subtropical_high_obj["shape"]["contour_5880gpm"] = contour_coords
                if not enhanced_shape:
                    subtropical_high_obj["shape"]["description"] = "å‘ˆä¸œè¥¿å‘ä¼¸å±•çš„è„Šçº¿å½¢æ€"

            # 4.4 ç›¸å¯¹ä½ç½®å’Œç»¼åˆæè¿°
            high_pos = subtropical_high_obj["position"]["center_of_mass"]
            bearing, rel_pos_desc = self._calculate_bearing(
                tc_lat, tc_lon, high_pos["lat"], high_pos["lon"]
            )
            subtropical_high_obj["position"]["relative_to_tc"] = rel_pos_desc

            desc = (
                f"ä¸€ä¸ªå¼ºåº¦ä¸ºâ€œ{level}â€çš„å‰¯çƒ­å¸¦é«˜å‹ç³»ç»Ÿä½äºå°é£çš„{rel_pos_desc}ï¼Œ"
                f"å…¶ä¸»ä½“å½¢æ€ç¨³å®šï¼Œä¸ºå°é£æä¾›äº†ç¨³å®šçš„{steering_direction:.0f}Â°æ–¹å‘ã€"
                f"é€Ÿåº¦ä¸º{steering_speed:.1f} m/sçš„å¼•å¯¼æ°”æµã€‚"
            )

            subtropical_high_obj.update(
                {
                    "system_name": "SubtropicalHigh",
                    "description": desc,
                    "properties": {
                        "influence": "ä¸»å¯¼å°é£æœªæ¥è·¯å¾„",
                        "steering_flow": {
                            "speed_mps": round(steering_speed, 2),
                            "direction_deg": round(steering_direction, 1),
                            "vector_mps": {"u": round(u_steering, 2), "v": round(v_steering, 2)},
                        },
                    },
                }
            )
            return subtropical_high_obj
        except Exception as e:
            # print(f"âš ï¸ å¼•å¯¼ç³»ç»Ÿæå–å¤±è´¥: {e}")
            return None

    def extract_vertical_wind_shear(self, time_idx, tc_lat, tc_lon):
        """
        [æ·±åº¦é‡æ„] æå–å¹¶è§£è¯‘å‚ç›´é£åˆ‡å˜ã€‚
        """
        try:
            u200, v200 = self._get_data_at_level("u", 200, time_idx), self._get_data_at_level(
                "v", 200, time_idx
            )
            u850, v850 = self._get_data_at_level("u", 850, time_idx), self._get_data_at_level(
                "v", 850, time_idx
            )
            if any(x is None for x in [u200, v200, u850, v850]):
                return None

            lat_idx, lon_idx = (
                np.abs(self.lat - tc_lat).argmin(),
                np.abs(self.lon - tc_lon).argmin(),
            )
            shear_u = u200[lat_idx, lon_idx] - u850[lat_idx, lon_idx]
            shear_v = v200[lat_idx, lon_idx] - v850[lat_idx, lon_idx]
            shear_mag = np.sqrt(shear_u**2 + shear_v**2)

            if shear_mag < 5:
                level, impact = "å¼±", "éå¸¸æœ‰åˆ©äºå‘å±•"
            elif shear_mag < 10:
                level, impact = "ä¸­ç­‰", "åŸºæœ¬æœ‰åˆ©å‘å±•"
            else:
                level, impact = "å¼º", "æ˜¾è‘—æŠ‘åˆ¶å‘å±•"

            # æ–¹å‘å®šä¹‰ä¸ºé£ä»å“ªä¸ªæ–¹å‘æ¥
            direction_from = (np.degrees(np.arctan2(shear_u, shear_v)) + 180) % 360
            dir_desc, _ = self._bearing_to_desc(direction_from)

            desc = (
                f"å°é£æ ¸å¿ƒåŒºæ­£å—åˆ°æ¥è‡ª{dir_desc}æ–¹å‘ã€å¼ºåº¦ä¸ºâ€œ{level}â€çš„å‚ç›´é£åˆ‡å˜å½±å“ï¼Œ"
                f"å½“å‰é£åˆ‡å˜ç¯å¢ƒå¯¹å°é£çš„å‘å±•{impact.split(' ')[-1]}ã€‚"
            )

            return {
                "system_name": "VerticalWindShear",
                "description": desc,
                "position": {
                    "description": "åœ¨å°é£ä¸­å¿ƒç‚¹è®¡ç®—çš„200-850hPaé£çŸ¢é‡å·®",
                    "lat": tc_lat,
                    "lon": tc_lon,
                },
                "intensity": {"value": round(shear_mag.item(), 2), "unit": "m/s", "level": level},
                "shape": {
                    "description": f"ä¸€ä¸ªä»{dir_desc}æŒ‡å‘çš„çŸ¢é‡",
                    "vector_coordinates": self._get_vector_coords(tc_lat, tc_lon, shear_u, shear_v),
                },
                "properties": {
                    "direction_from_deg": round(direction_from.item(), 1),
                    "impact": impact,
                    "shear_vector_mps": {
                        "u": round(shear_u.item(), 2),
                        "v": round(shear_v.item(), 2),
                    },
                },
            }
        except Exception as e:
            # print(f"âš ï¸ å‚ç›´é£åˆ‡å˜æå–å¤±è´¥: {e}")
            return None

    def extract_ocean_heat_content(self, time_idx, tc_lat, tc_lon, radius_deg=2.0):
        """
        [æ·±åº¦é‡æ„] æå–å¹¶è§£è¯‘æµ·æ´‹çƒ­å«é‡ï¼ˆæµ·è¡¨æ¸©åº¦SSTè¿‘ä¼¼ï¼‰ã€‚
        """
        try:
            sst = self._get_sst_field(time_idx)
            if sst is None:
                return None

            region_mask = self._create_region_mask(tc_lat, tc_lon, radius_deg)
            sst_mean = np.nanmean(sst[region_mask])

            if sst_mean > 29:
                level, impact = "æé«˜", "ä¸ºçˆ†å‘æ€§å¢å¼ºæä¾›é¡¶çº§èƒ½é‡"
            elif sst_mean > 28:
                level, impact = "é«˜", "éå¸¸æœ‰åˆ©äºåŠ å¼º"
            elif sst_mean > 26.5:
                level, impact = "ä¸­ç­‰", "è¶³ä»¥ç»´æŒå¼ºåº¦"
            else:
                level, impact = "ä½", "èƒ½é‡ä¾›åº”ä¸è¶³ï¼Œå°†å¯¼è‡´å‡å¼±"

            desc = (
                f"å°é£ä¸‹æ–¹æµ·åŸŸçš„å¹³å‡æµ·è¡¨æ¸©åº¦ä¸º{sst_mean:.1f}Â°Cï¼Œæµ·æ´‹çƒ­å«é‡ç­‰çº§ä¸ºâ€œ{level}â€ï¼Œ"
                f"{impact}ã€‚"
            )

            contour_26_5 = self._get_contour_coords(sst, 26.5, tc_lon)

            # å¢å¼ºå½¢çŠ¶åˆ†æï¼šåˆ†ææš–æ°´åŒºåŸŸå½¢çŠ¶
            enhanced_shape = self._get_enhanced_shape_info(sst, 26.5, "high", tc_lat, tc_lon)

            shape_info = {
                "description": "26.5Â°Cæ˜¯å°é£å‘å±•çš„æœ€ä½æµ·æ¸©é—¨æ§›ï¼Œæ­¤çº¿æ˜¯ç”Ÿå‘½çº¿",
                "warm_water_boundary_26.5C": contour_26_5,
            }

            # å¦‚æœæœ‰å¢å¼ºå½¢çŠ¶åˆ†æï¼Œæ·»åŠ æ›´å¤šç»†èŠ‚
            if enhanced_shape:
                shape_info.update(
                    {
                        "warm_water_area_km2": enhanced_shape["area_km2"],
                        "warm_region_shape": enhanced_shape["shape_type"],
                        "warm_region_orientation": enhanced_shape["orientation"],
                        "detailed_analysis": enhanced_shape["detailed_analysis"],
                    }
                )

                # æ›´æ–°æè¿°ä¿¡æ¯
                desc += f" æš–æ°´åŒºåŸŸé¢ç§¯çº¦{enhanced_shape['area_km2']:.0f}kmÂ²ï¼Œå‘ˆ{enhanced_shape['shape_type']}ï¼Œ{enhanced_shape['orientation']}ã€‚"

            return {
                "system_name": "OceanHeatContent",
                "description": desc,
                "position": {
                    "description": f"å°é£ä¸­å¿ƒå‘¨å›´{radius_deg}åº¦åŠå¾„å†…çš„æµ·åŸŸ",
                    "lat": tc_lat,
                    "lon": tc_lon,
                },
                "intensity": {"value": round(sst_mean.item(), 2), "unit": "Â°C", "level": level},
                "shape": shape_info,
                "properties": {"impact": impact},
            }
        except Exception as e:
            # print(f"âš ï¸ æµ·æ´‹çƒ­å«é‡æå–å¤±è´¥: {e}")
            return None

    def extract_upper_level_divergence(self, time_idx, tc_lat, tc_lon):
        """
        æå–å¹¶è§£è¯‘é«˜ç©ºè¾æ•£ç³»ç»Ÿï¼ˆ200hPaæ•£åº¦åœºï¼‰ã€‚
        é«˜ç©ºè¾æ•£æœ‰åˆ©äºä½å±‚è¾åˆåŠ å¼ºï¼Œä¿ƒè¿›å°é£å‘å±•ã€‚
        """
        try:
            u200 = self._get_data_at_level("u", 200, time_idx)
            v200 = self._get_data_at_level("v", 200, time_idx)
            if u200 is None or v200 is None:
                return None

            # è®¡ç®—æ•£åº¦åœº (åŠ å…¥æåŒºé˜²æŠ¤å’Œæœ‰é™å€¼è¿‡æ»¤)
            with np.errstate(divide="ignore", invalid="ignore"):
                gy_u, gx_u = self._raw_gradients(u200)
                gy_v, gx_v = self._raw_gradients(v200)
                du_dx = gx_u / (self.lon_spacing * 111000 * self._coslat_safe[:, np.newaxis])
                dv_dy = gy_v / (self.lat_spacing * 111000)
                divergence = du_dx + dv_dy
            if not np.any(np.isfinite(divergence)):
                return None
            divergence[~np.isfinite(divergence)] = np.nan

            lat_idx, lon_idx = self._loc_idx(tc_lat, tc_lon)
            div_val_raw = divergence[lat_idx, lon_idx]
            if not np.isfinite(div_val_raw):
                # ä½¿ç”¨å‘¨å›´ 3x3 æœ‰é™å€¼å¹³å‡æ›¿ä»£
                r = 1
                sub = divergence[max(0, lat_idx-r):lat_idx+r+1, max(0, lon_idx-r):lon_idx+r+1]
                finite_sub = sub[np.isfinite(sub)]
                if finite_sub.size == 0:
                    return None
                div_val_raw = float(np.nanmean(finite_sub))
            # åˆç†èŒƒå›´è£å‰ª (å…¸å‹æ•£åº¦é‡çº§ < 2e-4 s^-1)
            div_val_raw = float(np.clip(div_val_raw, -5e-4, 5e-4))
            div_value = div_val_raw * 1e5  # è½¬æ¢ä¸º10^-5 s^-1å•ä½

            if div_value > 5:
                level, impact = "å¼º", "æå…¶æœ‰åˆ©äºå°é£å‘å±•å’ŒåŠ å¼º"
            elif div_value > 2:
                level, impact = "ä¸­ç­‰", "æœ‰åˆ©äºå°é£ç»´æŒå’Œå‘å±•"
            elif div_value > -2:
                level, impact = "å¼±", "å¯¹å°é£å‘å±•å½±å“è¾ƒå°"
            else:
                level, impact = "è´Ÿå€¼", "ä¸åˆ©äºå°é£å‘å±•"

            desc = (
                f"å°é£ä¸Šæ–¹200hPaé«˜åº¦çš„æ•£åº¦å€¼ä¸º{div_value:.1f}Ã—10â»âµ sâ»Â¹ï¼Œé«˜ç©ºè¾æ•£å¼ºåº¦ä¸º'{level}'ï¼Œ"
                f"{impact}ã€‚"
            )

            return {
                "system_name": "UpperLevelDivergence",
                "description": desc,
                "position": {"description": "å°é£ä¸­å¿ƒä¸Šæ–¹200hPaé«˜åº¦", "lat": tc_lat, "lon": tc_lon},
                "intensity": {"value": round(div_value, 2), "unit": "Ã—10â»âµ sâ»Â¹", "level": level},
                "shape": {"description": "é«˜ç©ºè¾æ•£ä¸­å¿ƒçš„ç©ºé—´åˆ†å¸ƒ"},
                "properties": {"impact": impact, "favorable_development": div_value > 0},
            }
        except Exception as e:
            return None

    def extract_intertropical_convergence_zone(self, time_idx, tc_lat, tc_lon):
        """
        æå–å¹¶è§£è¯‘çƒ­å¸¦è¾åˆå¸¦(ITCZ)ã€‚
        ITCZæ˜¯çƒ­å¸¦å¯¹æµæ´»åŠ¨çš„ä¸»è¦åŒºåŸŸï¼Œå½±å“å°é£çš„ç”Ÿæˆå’Œè·¯å¾„ã€‚
        """
        try:
            u850 = self._get_data_at_level("u", 850, time_idx)
            v850 = self._get_data_at_level("v", 850, time_idx)
            if u850 is None or v850 is None:
                return None

            # è®¡ç®—850hPaæ¶¡åº¦æ¥è¯†åˆ«ITCZ
            gy_u, gx_u = self._raw_gradients(u850)
            gy_v, gx_v = self._raw_gradients(v850)
            du_dy = gy_u / (self.lat_spacing * 111000)
            dv_dx = gx_v / (self.lon_spacing * 111000 * self._coslat_safe[:, np.newaxis])
            vorticity = dv_dx - du_dy

            # ITCZé€šå¸¸ä½äº5Â°N-15Â°Nä¹‹é—´ï¼Œå¯»æ‰¾æœ€å¤§æ¶¡åº¦å¸¦
            tropical_mask = (self.lat >= 0) & (self.lat <= 20)
            if not np.any(tropical_mask):
                return None

            tropical_vort = vorticity[tropical_mask, :]
            max_vort_lat_idx = np.unravel_index(np.nanargmax(tropical_vort), tropical_vort.shape)[0]
            itcz_lat = self.lat[tropical_mask][max_vort_lat_idx]

            distance_to_tc = abs(tc_lat - itcz_lat)
            if distance_to_tc < 5:
                influence = "ç›´æ¥å½±å“å°é£å‘å±•"
            elif distance_to_tc < 10:
                influence = "å¯¹å°é£è·¯å¾„æœ‰æ˜¾è‘—å½±å“"
            else:
                influence = "å¯¹å°é£å½±å“è¾ƒå°"

            desc = f"çƒ­å¸¦è¾åˆå¸¦å½“å‰ä½äºçº¦{itcz_lat:.1f}Â°Né™„è¿‘ï¼Œä¸å°é£ä¸­å¿ƒè·ç¦»{distance_to_tc:.1f}åº¦ï¼Œ{influence}ã€‚"

            return {
                "system_name": "InterTropicalConvergenceZone",
                "description": desc,
                "position": {
                    "description": f"çƒ­å¸¦è¾åˆå¸¦ä½ç½®",
                    "lat": round(itcz_lat, 1),
                    "lon": "è·¨ç»åº¦å¸¦",
                },
                "intensity": {"description": "åŸºäº850hPaæ¶¡åº¦ç¡®å®šçš„æ´»è·ƒç¨‹åº¦"},
                "shape": {"description": "ä¸œè¥¿å‘å»¶ä¼¸çš„è¾åˆå¸¦"},
                "properties": {
                    "distance_to_tc_deg": round(distance_to_tc, 1),
                    "influence": influence,
                },
            }
        except Exception as e:
            return None

    def extract_westerly_trough(self, time_idx, tc_lat, tc_lon):
        """
        æå–å¹¶è§£è¯‘è¥¿é£æ§½ç³»ç»Ÿã€‚
        è¥¿é£æ§½å¯ä»¥ä¸ºå°é£æä¾›é¢å¤–çš„åŠ¨åŠ›æ”¯æŒæˆ–å½±å“å…¶è·¯å¾„ã€‚
        """
        try:
            z500 = self._get_data_at_level("z", 500, time_idx)
            if z500 is None:
                return None

            # å¯»æ‰¾ä¸­çº¬åº¦åœ°åŒºçš„æ§½çº¿ï¼ˆä½åŠ¿é«˜åº¦ç›¸å¯¹ä½å€¼åŒºï¼‰
            mid_lat_mask = (self.lat >= 20) & (self.lat <= 60)
            if not np.any(mid_lat_mask):
                return None

            # å¯»æ‰¾500hPaé«˜åº¦åœºçš„æ³¢åŠ¨
            z500_mid = z500[mid_lat_mask, :]
            trough_threshold = np.percentile(z500_mid, 25)  # å¯»æ‰¾ä½å››åˆ†ä½æ•°åŒºåŸŸ

            trough_systems = self._identify_pressure_system(
                z500, tc_lat, tc_lon, "low", trough_threshold
            )
            if not trough_systems:
                return None

            trough_lat = trough_systems["position"]["center_of_mass"]["lat"]
            trough_lon = trough_systems["position"]["center_of_mass"]["lon"]

            # è®¡ç®—ä¸å°é£çš„ç›¸å¯¹ä½ç½®
            bearing, rel_pos_desc = self._calculate_bearing(tc_lat, tc_lon, trough_lat, trough_lon)
            distance = self._calculate_distance(tc_lat, tc_lon, trough_lat, trough_lon)

            if distance < 1000:
                influence = "ç›´æ¥å½±å“å°é£è·¯å¾„å’Œå¼ºåº¦"
            elif distance < 2000:
                influence = "å¯¹å°é£æœ‰é—´æ¥å½±å“"
            else:
                influence = "å½±å“è¾ƒå°"

            desc = f"åœ¨å°é£{rel_pos_desc}çº¦{distance:.0f}å…¬é‡Œå¤„å­˜åœ¨è¥¿é£æ§½ç³»ç»Ÿï¼Œ{influence}ã€‚"

            # æ·»åŠ è¯¦ç»†çš„åæ ‡ä¿¡æ¯
            trough_coords = self._get_system_coordinates(
                z500, trough_threshold, "low", max_points=12
            )
            shape_info = {"description": "å—åŒ—å‘å»¶ä¼¸çš„æ§½çº¿ç³»ç»Ÿ"}

            if trough_coords:
                shape_info.update(
                    {
                        "coordinates": trough_coords,
                        "extent_desc": f"çº¬åº¦è·¨åº¦{trough_coords['span_deg'][1]:.1f}Â°ï¼Œç»åº¦è·¨åº¦{trough_coords['span_deg'][0]:.1f}Â°",
                    }
                )
                desc += f" æ§½çº¿ä¸»ä½“è·¨è¶Šçº¬åº¦{trough_coords['span_deg'][1]:.1f}Â°ï¼Œç»åº¦{trough_coords['span_deg'][0]:.1f}Â°ã€‚"

            return {
                "system_name": "WesterlyTrough",
                "description": desc,
                "position": trough_systems["position"],
                "intensity": trough_systems["intensity"],
                "shape": shape_info,
                "properties": {
                    "distance_to_tc_km": round(distance, 0),
                    "bearing_from_tc": round(bearing, 1),
                    "influence": influence,
                },
            }
        except Exception as e:
            return None

    def extract_frontal_system(self, time_idx, tc_lat, tc_lon):
        """
        æå–å¹¶è§£è¯‘é”‹é¢ç³»ç»Ÿã€‚
        é”‹é¢ç³»ç»Ÿé€šè¿‡æ¸©åº¦æ¢¯åº¦å’Œé£åˆ‡å˜å½±å“å°é£çš„ç§»åŠ¨è·¯å¾„ã€‚
        """
        try:
            t850 = self._get_data_at_level("t", 850, time_idx)
            if t850 is None:
                return None

            # è½¬æ¢æ¸©åº¦å•ä½
            if np.nanmean(t850) > 200:
                t850 = t850 - 273.15

            # è®¡ç®—æ¸©åº¦æ¢¯åº¦æ¥è¯†åˆ«é”‹é¢ (é˜²æ­¢æåŒº cos(latitude)=0 å¯¼è‡´é™¤é›¶ -> inf)
            with np.errstate(divide="ignore", invalid="ignore"):
                gy_t, gx_t = self._raw_gradients(t850)
                dt_dy = gy_t / (self.lat_spacing * 111000)
                dt_dx = gx_t / (self.lon_spacing * 111000 * self._coslat_safe[:, np.newaxis])
                temp_gradient = np.sqrt(dt_dx**2 + dt_dy**2)

            # æ¸…ç†å¼‚å¸¸å€¼
            if not np.any(np.isfinite(temp_gradient)):
                return None
            temp_gradient[~np.isfinite(temp_gradient)] = np.nan

            # å¯»æ‰¾å¼ºæ¸©åº¦æ¢¯åº¦åŒºåŸŸï¼ˆé”‹é¢ç‰¹å¾ï¼‰
            front_threshold = np.percentile(temp_gradient, 90)  # å‰10%çš„å¼ºæ¢¯åº¦åŒºåŸŸ
            front_mask = temp_gradient > front_threshold

            if not np.any(front_mask):
                return None

            # å¯»æ‰¾ç¦»å°é£æœ€è¿‘çš„é”‹é¢
            lat_idx, lon_idx = self._loc_idx(tc_lat, tc_lon)
            search_radius = 50  # æœç´¢åŠå¾„æ ¼ç‚¹æ•°

            lat_start = max(0, lat_idx - search_radius)
            lat_end = min(len(self.lat), lat_idx + search_radius)
            lon_start = max(0, lon_idx - search_radius)
            lon_end = min(len(self.lon), lon_idx + search_radius)

            local_front = front_mask[lat_start:lat_end, lon_start:lon_end]
            if not np.any(local_front):
                return None

            # ä½¿ç”¨æœ‰é™å€¼çš„æœ€å¤§å€¼
            finite_vals = temp_gradient[front_mask][np.isfinite(temp_gradient[front_mask])]
            if finite_vals.size == 0:
                return None
            front_strength = np.max(finite_vals)

            # æ•°å€¼åˆç†æ€§é™åˆ¶ï¼Œæç«¯æƒ…å†µè£å‰ªï¼Œå•ä½: Â°C/m
            if not np.isfinite(front_strength) or front_strength <= 0:
                return None
            # å…¸å‹é”‹é¢æ°´å¹³æ¸©åº¦æ¢¯åº¦ ~ 1e-5 åˆ° æ•°å€¼æ¨¡å¼ä¸­å°‘è§è¶…è¿‡ 1e-4
            front_strength = float(np.clip(front_strength, 0, 5e-4))

            if front_strength > 3e-5:
                level = "å¼º"
            elif front_strength > 1e-5:
                level = "ä¸­ç­‰"
            else:
                level = "å¼±"

            strength_1e5 = front_strength * 1e5  # è½¬æ¢ä¸º Ã—10â»âµ Â°C/m æ ‡åº¦
            desc = (
                f"å°é£å‘¨å›´å­˜åœ¨å¼ºåº¦ä¸º'{level}'çš„é”‹é¢ç³»ç»Ÿï¼Œæ¸©åº¦æ¢¯åº¦è¾¾åˆ°{strength_1e5:.1f}Ã—10â»âµ Â°C/mï¼Œ"
                f"å¯èƒ½å½±å“å°é£çš„ç§»åŠ¨è·¯å¾„ã€‚"
            )

            # æå–é”‹é¢å¸¦çš„åæ ‡ä¿¡æ¯
            frontal_coords = self._get_system_coordinates(
                temp_gradient, front_threshold, "high", max_points=15
            )
            shape_info = {"description": "çº¿æ€§çš„æ¸©åº¦æ¢¯åº¦å¸¦"}

            if frontal_coords:
                shape_info.update(
                    {
                        "coordinates": frontal_coords,
                        "extent_desc": f"é”‹é¢å¸¦è·¨è¶Šçº¬åº¦{frontal_coords['span_deg'][1]:.1f}Â°ï¼Œç»åº¦{frontal_coords['span_deg'][0]:.1f}Â°",
                        "orientation_note": "æ ¹æ®å‡ ä½•å½¢çŠ¶ç¡®å®šé”‹é¢èµ°å‘",
                    }
                )
                desc += f" é”‹é¢å¸¦ä¸»ä½“è·¨è¶Š{frontal_coords['span_deg'][1]:.1f}Â°çº¬åº¦å’Œ{frontal_coords['span_deg'][0]:.1f}Â°ç»åº¦ã€‚"

            return {
                "system_name": "FrontalSystem",
                "description": desc,
                "position": {"description": "å°é£å‘¨å›´çš„é”‹é¢åŒºåŸŸ", "lat": tc_lat, "lon": tc_lon},
                "intensity": {
                    "value": round(strength_1e5, 2),
                    "unit": "Ã—10â»âµ Â°C/m",
                    "level": level,
                },
                "shape": shape_info,
                "properties": {"impact": "å½±å“å°é£è·¯å¾„å’Œç»“æ„"},
            }
        except Exception as e:
            return None

    def extract_monsoon_trough(self, time_idx, tc_lat, tc_lon):
        """
        æå–å¹¶è§£è¯‘å­£é£æ§½ç³»ç»Ÿã€‚
        å­£é£æ§½æ˜¯çƒ­å¸¦æ°”æ—‹ç”Ÿæˆçš„é‡è¦ç¯å¢ƒï¼Œä¹Ÿå½±å“ç°æœ‰å°é£çš„å‘å±•ã€‚
        """
        try:
            u850 = self._get_data_at_level("u", 850, time_idx)
            v850 = self._get_data_at_level("v", 850, time_idx)
            if u850 is None or v850 is None:
                return None

            # è®¡ç®—850hPaç›¸å¯¹æ¶¡åº¦
            gy_u, gx_u = self._raw_gradients(u850)
            gy_v, gx_v = self._raw_gradients(v850)
            du_dy = gy_u / (self.lat_spacing * 111000)
            dv_dx = gx_v / (self.lon_spacing * 111000 * self._coslat_safe[:, np.newaxis])
            relative_vorticity = dv_dx - du_dy

            # æ¸…ç†å¼‚å¸¸æ•°å€¼
            with np.errstate(invalid="ignore"):
                relative_vorticity[~np.isfinite(relative_vorticity)] = np.nan

            # å­£é£æ§½é€šå¸¸åœ¨çƒ­å¸¦åœ°åŒºï¼Œå¯»æ‰¾æ­£æ¶¡åº¦å¸¦
            tropical_mask = (self.lat >= -30) & (self.lat <= 30)
            if not np.any(tropical_mask):
                return None

            tropical_vort = relative_vorticity[tropical_mask, :]
            monsoon_threshold = (
                np.percentile(tropical_vort[tropical_vort > 0], 75)
                if np.any(tropical_vort > 0)
                else 0
            )

            if monsoon_threshold <= 0:
                return None

            monsoon_mask = relative_vorticity > monsoon_threshold
            lat_idx, lon_idx = self._loc_idx(tc_lat, tc_lon)

            # æ£€æŸ¥å°é£é™„è¿‘æ˜¯å¦å­˜åœ¨å­£é£æ§½
            search_radius = 30
            lat_start = max(0, lat_idx - search_radius)
            lat_end = min(len(self.lat), lat_idx + search_radius)
            lon_start = max(0, lon_idx - search_radius)
            lon_end = min(len(self.lon), lon_idx + search_radius)

            local_monsoon = monsoon_mask[lat_start:lat_end, lon_start:lon_end]
            if not np.any(local_monsoon):
                return None

            finite_vort = relative_vorticity[monsoon_mask][
                np.isfinite(relative_vorticity[monsoon_mask])
            ]
            if finite_vort.size == 0:
                return None
            max_vorticity = float(np.max(finite_vort))
            # è£å‰ªåˆ°åˆç†èŒƒå›´ (å…¸å‹çƒ­å¸¦æ¶¡åº¦ < 2e-3 s^-1)
            max_vorticity = float(np.clip(max_vorticity, 0, 2e-3)) * 1e5

            if max_vorticity > 10:
                level, impact = "æ´»è·ƒ", "ä¸ºå°é£å‘å±•æä¾›æœ‰åˆ©ç¯å¢ƒ"
            elif max_vorticity > 5:
                level, impact = "ä¸­ç­‰", "å¯¹å°é£å‘å±•æœ‰ä¸€å®šæ”¯æŒ"
            else:
                level, impact = "å¼±", "å¯¹å°é£å½±å“æœ‰é™"

            desc = (
                f"å°é£å‘¨å›´å­˜åœ¨æ´»è·ƒç¨‹åº¦ä¸º'{level}'çš„å­£é£æ§½ç³»ç»Ÿï¼Œæœ€å¤§ç›¸å¯¹æ¶¡åº¦ä¸º{max_vorticity:.1f}Ã—10â»âµ sâ»Â¹ï¼Œ"
                f"{impact}ã€‚"
            )

            return {
                "system_name": "MonsoonTrough",
                "description": desc,
                "position": {"description": "å°é£å‘¨å›´çš„å­£é£æ§½åŒºåŸŸ", "lat": tc_lat, "lon": tc_lon},
                "intensity": {
                    "value": round(max_vorticity, 1),
                    "unit": "Ã—10â»âµ sâ»Â¹",
                    "level": level,
                },
                "shape": {"description": "ä¸œè¥¿å‘å»¶ä¼¸çš„ä½å‹æ§½"},
                "properties": {"impact": impact, "vorticity_support": max_vorticity > 5},
            }
        except Exception as e:
            return None

    # --- ä¸»åˆ†æä¸å¯¼å‡ºå‡½æ•° ---
    def analyze_and_export_as_json(self, output_dir="final_output"):
        # ... (æ­¤å‡½æ•°é€»è¾‘ä¸ä¸Šä¸€ç‰ˆåŸºæœ¬ç›¸åŒï¼Œæ— éœ€ä¿®æ”¹) ...
        print("\nğŸ” å¼€å§‹è¿›è¡Œä¸“å®¶çº§ç¯å¢ƒåœºè§£è¯‘å¹¶æ„å»ºJSON...")
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # === æ–°å¢: å¦‚æœè¾“å‡ºå·²å­˜åœ¨åˆ™è·³è¿‡é‡ç®— ===
        # åˆ¤å®šæ ‡å‡†: å¯¹å½“å‰ NC æ–‡ä»¶ (self.nc_stem) æ‰€æœ‰ç²’å­(è‹¥æ— ç²’å­åˆ—åˆ™é»˜è®¤ä¸º TC_01) çš„
        # ç›®æ ‡æ–‡ä»¶ <ncstem>_TC_Analysis_<particle>.json å‡å·²å­˜åœ¨ä¸”éç©º, åˆ™ç›´æ¥è·³è¿‡
        existing_outputs = list(output_path.glob(f"{self.nc_stem}_TC_Analysis_*.json"))
        if existing_outputs:
            # ç¡®å®šæœŸæœ›ç²’å­é›†åˆ
            if "particle" in self.tc_tracks.columns:
                expected_particles = sorted(set(str(p) for p in self.tc_tracks["particle"].unique()))
            else:
                expected_particles = ["TC_01"]
            # å·²å­˜åœ¨å¹¶ä¸”æ–‡ä»¶éç©ºçš„ç²’å­ç»“æœ
            existing_particles = []
            for pfile in existing_outputs:
                # æ–‡ä»¶åæ ¼å¼: <ncstem>_TC_Analysis_<pid>.json -> æå– <pid>
                stem = pfile.stem
                if stem.startswith(f"{self.nc_stem}_TC_Analysis_"):
                    pid = stem.replace(f"{self.nc_stem}_TC_Analysis_", "")
                    try:
                        if pfile.stat().st_size > 10:  # ç®€å•åˆ¤å®šéç©º
                            existing_particles.append(pid)
                    except Exception:
                        pass
            if set(expected_particles).issubset(existing_particles):
                print(
                    f"â© æ£€æµ‹åˆ°å½“å‰NCå¯¹åº”çš„æ‰€æœ‰åˆ†æç»“æœå·²å­˜åœ¨äº '{output_path}' (å…±{len(existing_particles)}ä¸ª)ï¼Œè·³è¿‡é‡ç®—ã€‚"
                )
                return {pid: None for pid in expected_particles}  # è¿”å›å ä½, è¡¨ç¤ºå·²è·³è¿‡

        if "particle" not in self.tc_tracks.columns:
            print("è­¦å‘Š: è·¯å¾„æ–‡ä»¶ .csv ä¸­æœªæ‰¾åˆ° 'particle' åˆ—ï¼Œå°†æ‰€æœ‰è·¯å¾„ç‚¹è§†ä¸ºå•ä¸ªå°é£äº‹ä»¶ã€‚")
            self.tc_tracks["particle"] = "TC_01"

        tc_groups = self.tc_tracks.groupby("particle")
        all_typhoon_events = {}

        for tc_id, track_df in tc_groups:
            print(f"\nğŸŒ€ æ­£åœ¨å¤„ç†å°é£äº‹ä»¶: {tc_id}")
            event_data = {
                "tc_id": str(tc_id),
                "analysis_time": datetime.now().isoformat(),
                "time_series": [],
            }

            for _, track_point in track_df.sort_values(by="time").iterrows():
                time_idx, lat, lon = (
                    int(track_point.get("time_idx", 0)),
                    track_point["lat"],
                    track_point["lon"],
                )
                print(f"  -> åˆ†ææ—¶é—´ç‚¹: {track_point['time'].strftime('%Y-%m-%d %H:%M')}")

                environmental_systems = []
                systems_to_extract = [
                    self.extract_steering_system,
                    self.extract_vertical_wind_shear,
                    self.extract_ocean_heat_content,
                    self.extract_upper_level_divergence,
                    self.extract_intertropical_convergence_zone,
                    self.extract_westerly_trough,
                    self.extract_frontal_system,
                    self.extract_monsoon_trough,
                ]

                for func in systems_to_extract:
                    system_obj = func(time_idx, lat, lon)
                    if system_obj:
                        environmental_systems.append(system_obj)

                event_data["time_series"].append(
                    {
                        "time": track_point["time"].isoformat(),
                        "time_idx": time_idx,
                        "tc_position": {"lat": lat, "lon": lon},
                        "tc_intensity_hpa": track_point.get("intensity", None),
                        "environmental_systems": environmental_systems,
                    }
                )
            all_typhoon_events[str(tc_id)] = event_data

        for tc_id, data in all_typhoon_events.items():
            # åœ¨è¾“å‡ºæ–‡ä»¶åä¸­åŠ å…¥åŸå§‹NCæ–‡ä»¶å(å»æ‰©å±•)ï¼Œæ ¼å¼: <ncstem>_TC_Analysis_<tc_id>.json
            json_filename = output_path / f"{self.nc_stem}_TC_Analysis_{tc_id}.json"
            print(f"ğŸ’¾ ä¿å­˜ä¸“å®¶è§£è¯‘ç»“æœåˆ°: {json_filename}")

            # é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            def convert_numpy_types(obj):
                if isinstance(obj, dict):
                    return {k: convert_numpy_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(v) for v in obj]
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, (np.float32, np.float64)):
                    val = float(obj)
                    if not np.isfinite(val):
                        return None
                    return val
                elif isinstance(obj, (np.int32, np.int64)):
                    return int(obj)
                elif isinstance(obj, np.bool_):
                    return bool(obj)
                else:
                    return obj

            # é¢å¤–é€’å½’å¤„ç† Python float ä¸­çš„ inf / nan
            def sanitize_inf_nan(o):
                if isinstance(o, dict):
                    return {k: sanitize_inf_nan(v) for k, v in o.items()}
                elif isinstance(o, list):
                    return [sanitize_inf_nan(v) for v in o]
                elif isinstance(o, float):
                    if math.isinf(o) or math.isnan(o):
                        return None
                    return o
                return o

            converted_data = convert_numpy_types(data)
            converted_data = sanitize_inf_nan(converted_data)

            with open(json_filename, "w", encoding="utf-8") as f:
                json.dump(converted_data, f, indent=4, ensure_ascii=False)

        print(f"\nâœ… æ‰€æœ‰å°é£äº‹ä»¶è§£è¯‘å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {output_path}")
        return all_typhoon_events

    # --- è¾…åŠ©ä¸å·¥å…·å‡½æ•° ---
    def _get_sst_field(self, time_idx):
        # ä¼˜å…ˆæŸ¥æ‰¾SSTæ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨2ç±³æ¸©åº¦ä½œä¸ºè¿‘ä¼¼
        for var_name in ["sst", "ts"]:
            if var_name in self.ds.data_vars:
                sst_data = self.ds[var_name].isel(time=time_idx).values
                return sst_data - 273.15 if np.nanmean(sst_data) > 200 else sst_data

        # å¦‚æœæ²¡æœ‰SSTæ•°æ®ï¼Œä½¿ç”¨2ç±³æ¸©åº¦ä½œä¸ºè¿‘ä¼¼ï¼ˆä»…åœ¨æµ·æ´‹åŒºåŸŸæœ‰æ•ˆï¼‰
        for var_name in ["t2", "t2m"]:
            if var_name in self.ds.data_vars:
                t2_data = self.ds[var_name].isel(time=time_idx).values
                # è½¬æ¢æ¸©åº¦å•ä½
                sst_approx = t2_data - 273.15 if np.nanmean(t2_data) > 200 else t2_data
                # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼ï¼Œåœ¨é™†åœ°ä¸Šä¼šä¸å‡†ç¡®
                print(f"âš ï¸  ä½¿ç”¨{var_name}ä½œä¸ºæµ·è¡¨æ¸©åº¦è¿‘ä¼¼")
                return sst_approx

        return None

    def _calculate_steering_flow(self, z500, tc_lat, tc_lon):
        gy, gx = self._raw_gradients(z500)
        dy = gy / (self.lat_spacing * 111000)
        dx = gx / (self.lon_spacing * 111000 * self._coslat_safe[:, np.newaxis])
        lat_idx, lon_idx = self._loc_idx(tc_lat, tc_lon)
        u_steering = -dx[lat_idx, lon_idx] / (9.8 * 1e-5)
        v_steering = dy[lat_idx, lon_idx] / (9.8 * 1e-5)
        speed = np.sqrt(u_steering**2 + v_steering**2)
        direction = (np.degrees(np.arctan2(u_steering, v_steering)) + 180) % 360
        return speed, direction, u_steering, v_steering

    def _get_contour_coords(self, data_field, level, center_lon, max_points=100):
        try:
            contours = find_contours(data_field, level)
            if not contours:
                return None
            # å¯»æ‰¾æœ€é•¿çš„ç­‰å€¼çº¿æ®µï¼Œé€šå¸¸æ˜¯ä¸»ç³»ç»Ÿ
            main_contour = sorted(contours, key=len, reverse=True)[0]

            # å¯¹ç»åº¦è¿›è¡Œæ­£ç¡®è½¬æ¢
            contour_lon = self.lon[main_contour[:, 1].astype(int)]
            contour_lat = self.lat[main_contour[:, 0].astype(int)]

            # é™é‡‡æ ·ä»¥å‡å°‘æ•°æ®é‡
            step = max(1, len(main_contour) // max_points)
            return [
                [round(lon, 2), round(lat, 2)]
                for lon, lat in zip(contour_lon[::step], contour_lat[::step])
            ]
        except Exception:
            return None

    def _get_enhanced_shape_info(self, data_field, threshold, system_type, center_lat, center_lon):
        """
        è·å–å¢å¼ºçš„å½¢çŠ¶ä¿¡æ¯ï¼ŒåŒ…å«è¯¦ç»†çš„åæ ‡å®šä½
        """
        try:
            shape_analysis = self.shape_analyzer.analyze_system_shape(
                data_field, threshold, system_type, center_lat, center_lon
            )
            if shape_analysis:
                # åŸºç¡€ä¿¡æ¯
                basic_info = {
                    "area_km2": shape_analysis["basic_geometry"]["area_km2"],
                    "shape_type": shape_analysis["basic_geometry"]["description"],
                    "orientation": shape_analysis["orientation"]["direction_type"],
                    "complexity": shape_analysis["shape_complexity"]["description"],
                    "detailed_analysis": shape_analysis,
                }

                # æ·»åŠ åæ ‡ä¿¡æ¯
                if "contour_analysis" in shape_analysis and shape_analysis["contour_analysis"]:
                    contour_data = shape_analysis["contour_analysis"]
                    basic_info.update(
                        {
                            "coordinate_info": {
                                "main_contour_coords": contour_data.get(
                                    "simplified_coordinates", []
                                ),
                                "polygon_features": contour_data.get("polygon_features", {}),
                                "contour_length_km": contour_data.get("contour_length_km", 0),
                            }
                        }
                    )

                return basic_info
        except Exception as e:
            print(f"å½¢çŠ¶åˆ†æå¤±è´¥: {e}")
        return None

    def _get_system_coordinates(self, data_field, threshold, system_type, max_points=20):
        """
        ä¸“é—¨æå–æ°”è±¡ç³»ç»Ÿçš„å…³é”®åæ ‡ç‚¹
        """
        try:
            # åˆ›å»ºç³»ç»Ÿæ©è†œ
            if system_type == "high":
                mask = data_field >= threshold
            else:
                mask = data_field <= threshold

            if not np.any(mask):
                return None

            # æ‰¾åˆ°è¿é€šåŒºåŸŸ
            labeled_mask, num_features = label(mask)
            if num_features == 0:
                return None

            # é€‰æ‹©æœ€å¤§çš„è¿é€šåŒºåŸŸ
            flat_labels = labeled_mask.ravel()
            counts = np.bincount(flat_labels)[1: num_features + 1]
            if counts.size == 0:
                return None
            main_label = int(np.argmax(counts) + 1)
            main_region = labeled_mask == main_label

            # æå–è¾¹ç•Œåæ ‡
            contours = find_contours(main_region.astype(float), 0.5)
            if not contours:
                return None

            main_contour = max(contours, key=len)

            # ç®€åŒ–å¤šè¾¹å½¢ä»¥è·å¾—å…³é”®ç‚¹
            epsilon = len(main_contour) * 0.01  # ç®€åŒ–ç¨‹åº¦
            simplified = approximate_polygon(main_contour, tolerance=epsilon)

            # é™åˆ¶ç‚¹æ•°
            if len(simplified) > max_points:
                step = len(simplified) // max_points
                simplified = simplified[::step]

            # è½¬æ¢ä¸ºåœ°ç†åæ ‡
            geo_coords = []
            for point in simplified:
                lat_idx = int(np.clip(point[0], 0, len(self.lat) - 1))
                lon_idx = int(np.clip(point[1], 0, len(self.lon) - 1))
                # ä½¿ç”¨æ›´ç´§å‡‘çš„æ•°ç»„æ ¼å¼ [lon, lat]
                geo_coords.append([round(self.lon[lon_idx], 3), round(self.lat[lat_idx], 3)])

            # è®¡ç®—ç³»ç»ŸèŒƒå›´
            if geo_coords:
                lons = [coord[0] for coord in geo_coords]
                lats = [coord[1] for coord in geo_coords]

                extent = {
                    "boundaries": [
                        round(min(lons), 3),
                        round(min(lats), 3),
                        round(max(lons), 3),
                        round(max(lats), 3),
                    ],  # [west, south, east, north]
                    "center": [round(np.mean(lons), 3), round(np.mean(lats), 3)],  # [lon, lat]
                    "span": [
                        round(max(lons) - min(lons), 3),
                        round(max(lats) - min(lats), 3),
                    ],  # [lon_span, lat_span]
                }

                return {
                    "vertices": geo_coords,  # ç®€åŒ–çš„æ•°ç»„æ ¼å¼
                    "vertex_count": len(geo_coords),
                    "extent": extent,
                    "span_deg": [extent["span"][0], extent["span"][1]],  # [lon_span, lat_span]
                }

            return None
        except Exception as e:
            print(f"åæ ‡æå–å¤±è´¥: {e}")
            return None

    def _generate_coordinate_description(self, coords_info, system_name="ç³»ç»Ÿ"):
        """
        ç”Ÿæˆå¯è¯»çš„åæ ‡æè¿°æ–‡æœ¬
        """
        if not coords_info:
            return ""

        try:
            description_parts = []

            # ç³»ç»ŸèŒƒå›´æè¿°
            if "extent" in coords_info:
                extent = coords_info["extent"]
                boundaries = extent["boundaries"]  # [west, south, east, north]
                description_parts.append(
                    f"{system_name}ä¸»ä½“ä½äº{boundaries[0]:.1f}Â°E-{boundaries[2]:.1f}Â°Eï¼Œ"
                    f"{boundaries[1]:.1f}Â°N-{boundaries[3]:.1f}Â°N"
                )

            # å…³é”®é¡¶ç‚¹æè¿°
            if "vertices" in coords_info and coords_info["vertex_count"] > 0:
                vertex_count = coords_info["vertex_count"]
                description_parts.append(f"ç”±{vertex_count}ä¸ªå…³é”®é¡¶ç‚¹æ„æˆçš„å¤šè¾¹å½¢å½¢çŠ¶")

            # å°ºåº¦æè¿°
            if "span_deg" in coords_info:
                lon_span, lat_span = coords_info["span_deg"]
                lat_km = lat_span * 111  # çº¬åº¦1åº¦çº¦111km
                center_lat = coords_info.get("extent", {}).get("center", [0, 30])[1]
                lon_km = lon_span * 111 * np.cos(np.radians(center_lat))
                description_parts.append(f"çº¬å‘è·¨åº¦çº¦{lat_km:.0f}kmï¼Œç»å‘è·¨åº¦çº¦{lon_km:.0f}km")

            return "ï¼Œ".join(description_parts) + "ã€‚" if description_parts else ""

        except Exception:
            return ""

    def _calculate_distance(self, lat1, lon1, lat2, lon2):
        """è®¡ç®—ä¸¤ç‚¹é—´çš„çƒé¢è·ç¦»ï¼ˆå•ä½ï¼šå…¬é‡Œï¼‰"""
        R = 6371.0  # åœ°çƒåŠå¾„ï¼Œå…¬é‡Œ
        lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2
        c = 2 * math.asin(math.sqrt(a))
        return R * c

    def _calculate_bearing(self, lat1, lon1, lat2, lon2):
        dLon = math.radians(lon2 - lon1)
        lat1, lat2 = math.radians(lat1), math.radians(lat2)
        y = math.sin(dLon) * math.cos(lat2)
        x = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(dLon)
        bearing = (math.degrees(math.atan2(y, x)) + 360) % 360
        return bearing, self._bearing_to_desc(bearing)[1]

    def _bearing_to_desc(self, bearing):
        dirs = [
            "åŒ—",
            "ä¸œåŒ—ååŒ—",
            "ä¸œåŒ—",
            "ä¸œåŒ—åä¸œ",
            "ä¸œ",
            "ä¸œå—åä¸œ",
            "ä¸œå—",
            "ä¸œå—åå—",
            "å—",
            "è¥¿å—åå—",
            "è¥¿å—",
            "è¥¿å—åè¥¿",
            "è¥¿",
            "è¥¿åŒ—åè¥¿",
            "è¥¿åŒ—",
            "è¥¿åŒ—ååŒ—",
        ]
        wind_dirs = [
            "ååŒ—é£",
            "ä¸œåŒ—ååŒ—é£",
            "ä¸œåŒ—é£",
            "ä¸œåŒ—åä¸œé£",
            "åä¸œé£",
            "ä¸œå—åä¸œé£",
            "ä¸œå—é£",
            "ä¸œå—åå—é£",
            "åå—é£",
            "è¥¿å—åå—é£",
            "è¥¿å—é£",
            "è¥¿å—åè¥¿é£",
            "åè¥¿é£",
            "è¥¿åŒ—åè¥¿é£",
            "è¥¿åŒ—é£",
            "è¥¿åŒ—ååŒ—é£",
        ]
        index = round(bearing / 22.5) % 16
        return wind_dirs[index], f"{dirs[index]}æ–¹å‘"

    def _get_vector_coords(self, lat, lon, u, v, scale=0.1):
        # å°† m/s è½¬æ¢ä¸ºç»çº¬åº¦åç§»
        # è¿™æ˜¯ä¸€ä¸ªéå¸¸ç²—ç•¥çš„è¿‘ä¼¼ï¼Œä»…ç”¨äºå¯è§†åŒ–ç¤ºæ„
        end_lat = lat + v * scale * 0.009  # 1 m/s ~ 0.009 deg lat
        end_lon = lon + u * scale * 0.009 / math.cos(math.radians(lat))
        return {
            "start": {"lat": round(lat, 2), "lon": round(lon, 2)},
            "end": {"lat": round(end_lat, 2), "lon": round(end_lon, 2)},
        }

    def _identify_pressure_system(self, *args, **kwargs):
        # ... (æ­¤å‡½æ•°ä¸ä¸Šä¸€ç‰ˆç›¸åŒ) ...
        data_field, tc_lat, tc_lon, system_type, threshold = args
        if system_type == "high":
            mask = data_field > threshold
        else:
            mask = data_field < threshold
        if not np.any(mask):
            return None
        labeled_array, num_features = label(mask)
        if num_features == 0:
            return None
        objects_slices = find_objects(labeled_array)
        min_dist, closest_feature_idx = float("inf"), -1
        tc_lat_idx, tc_lon_idx = (
            np.abs(self.lat - tc_lat).argmin(),
            np.abs(self.lon - tc_lon).argmin(),
        )
        for i, slc in enumerate(objects_slices):
            center_y, center_x = (slc[0].start + slc[0].stop) / 2, (slc[1].start + slc[1].stop) / 2
            dist = np.sqrt((center_y - tc_lat_idx) ** 2 + (center_x - tc_lon_idx) ** 2)
            if dist < min_dist:
                min_dist, closest_feature_idx = dist, i
        if closest_feature_idx == -1:
            return None
        target_slc = objects_slices[closest_feature_idx]
        target_mask = labeled_array == (closest_feature_idx + 1)
        com_y, com_x = center_of_mass(target_mask)
        pos_lat, pos_lon = self.lat[int(com_y)], self.lon[int(com_x)]
        intensity_val = (
            np.max(data_field[target_mask])
            if system_type == "high"
            else np.min(data_field[target_mask])
        )
        lat_min, lat_max = self.lat[target_slc[0].start], self.lat[target_slc[0].stop - 1]
        lon_min, lon_max = self.lon[target_slc[1].start], self.lon[target_slc[1].stop - 1]
        return {
            "position": {
                "center_of_mass": {"lat": round(pos_lat.item(), 2), "lon": round(pos_lon.item(), 2)}
            },
            "intensity": {"value": round(intensity_val.item(), 1), "unit": "gpm"},
            "shape": {},
        }

    def _get_data_at_level(self, *args, **kwargs):
        # ... (æ­¤å‡½æ•°ä¸ä¸Šä¸€ç‰ˆç›¸åŒ) ...
        var_name, level_hPa, time_idx = args
        if var_name not in self.ds.data_vars:
            return None
        var_data = self.ds[var_name]
        level_dim = next(
            (dim for dim in ["level", "isobaricInhPa", "pressure"] if dim in var_data.dims), None
        )
        if level_dim is None:
            return (
                var_data.isel(time=time_idx).values if "time" in var_data.dims else var_data.values
            )
        levels = self.ds[level_dim].values
        level_idx = np.abs(levels - level_hPa).argmin()
        return var_data.isel(time=time_idx, **{level_dim: level_idx}).values

    def _create_region_mask(self, *args, **kwargs):
        # ... (æ­¤å‡½æ•°ä¸ä¸Šä¸€ç‰ˆç›¸åŒ) ...
        center_lat, center_lon, radius_deg = args
        lat_mask = (self.lat >= center_lat - radius_deg) & (self.lat <= center_lat + radius_deg)
        lon_mask = (self.lon >= center_lon - radius_deg) & (self.lon <= center_lon + radius_deg)
        return np.outer(lat_mask, lon_mask)


    # ================= æ–°å¢: æµå¼é¡ºåºå¤„ç†å‡½æ•° =================
def streaming_from_csv(
    csv_path: Path,
    limit: int | None = None,
    search_range: float = 3.0,
    memory: int = 3,
    keep_nc: bool = False,
    initials_csv: Path | None = None,
):
    """é€è¡Œè¯»å–CSV, æ¯ä¸ªNCæ–‡ä»¶æ‰§è¡Œ: ä¸‹è½½ -> è¿½è¸ª -> ç¯å¢ƒåˆ†æ -> (å¯é€‰åˆ é™¤)

    ä¸åŸæ‰¹é‡æ¨¡å¼æœ€å¤§åŒºåˆ«: ä¸é¢„å…ˆä¸‹è½½å…¨éƒ¨; æ¯ä¸ªæ–‡ä»¶å®Œæˆåå³å¯é‡Šæ”¾ç£ç›˜ã€‚
    """
    if not csv_path.exists():
        print(f"âŒ CSVä¸å­˜åœ¨: {csv_path}")
        return
    import pandas as pd, re, traceback
    from trackTC import sanitize_filename, download_s3_public
    # initialTracker æä¾›åŸºäºåˆå§‹ç‚¹çš„è¿½è¸ªç®—æ³•
    from initialTracker import track_file_with_initials as it_track_file_with_initials
    # å…¼å®¹: initialTracker ä¸­æä¾›çš„æ˜¯ _load_all_pointsï¼Œè¿™é‡Œç”¨åŒååˆ«åå¼•ç”¨
    from initialTracker import _load_all_points as it_load_initial_points

    df = pd.read_csv(csv_path)
    required_cols = {"s3_url", "model_prefix", "init_time"}
    if not required_cols.issubset(df.columns):
        print(f"âŒ CSVç¼ºå°‘å¿…è¦åˆ—: {required_cols - set(df.columns)}")
        return
    if limit is not None:
        df = df.head(limit)
    print(f"ğŸ“„ æµå¼å¾…å¤„ç†æ•°é‡: {len(df)} (limit={limit})")

    persist_dir = Path("data/nc_files")  # ä»æ”¾å…¥è¯¥ç›®å½•, ä¾¿äºå¤ç”¨é€»è¾‘
    persist_dir.mkdir(parents=True, exist_ok=True)
    track_dir = Path("track_test"); track_dir.mkdir(exist_ok=True)
    final_dir = Path("final_output"); final_dir.mkdir(exist_ok=True)

    processed = 0
    skipped = 0
    for idx, row in df.iterrows():
        s3_url = row["s3_url"]
        model_prefix = row["model_prefix"]
        init_time = row["init_time"]
        fname = Path(s3_url).name
        m = re.search(r"(f\d{3}_f\d{3}_\d{2})", Path(fname).stem)
        forecast_tag = m.group(1) if m else "track"
        safe_prefix = sanitize_filename(model_prefix)
        safe_init = sanitize_filename(init_time.replace(":", "").replace("-", ""))
        track_csv = track_dir / f"tracks_{safe_prefix}_{safe_init}_{forecast_tag}.csv"
        nc_local = persist_dir / fname

        print(f"\n[{idx+1}/{len(df)}] â–¶ï¸ å¤„ç†: {fname}")

        # å¦‚æœ final å·²å­˜åœ¨åˆ™è·³è¿‡æ•´ä¸ªæµç¨‹
        existing_json = list(final_dir.glob(f"{Path(fname).stem}_TC_Analysis_*.json"))
        if existing_json:
            non_empty = [p for p in existing_json if p.stat().st_size > 10]
            if non_empty:
                print(f"â­ï¸  å·²å­˜åœ¨æœ€ç»ˆJSON({len(non_empty)}) -> è·³è¿‡")
                skipped += 1
                continue

        # ä¸‹è½½ (è‹¥ä¸å­˜åœ¨)
        if not nc_local.exists():
            try:
                print(f"â¬‡ï¸  ä¸‹è½½NC: {s3_url}")
                download_s3_public(s3_url, nc_local)
            except Exception as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥, è·³è¿‡: {e}")
                skipped += 1
                continue
        else:
            print("ğŸ“¦ å·²å­˜åœ¨NCæ–‡ä»¶, å¤ç”¨")

        # è½¨è¿¹: è‹¥ä¸å­˜åœ¨åˆ™è®¡ç®— (ä½¿ç”¨ initialTracker)
        if not track_csv.exists():
            try:
                print("ğŸ§­ ä½¿ç”¨ initialTracker æ‰§è¡Œè¿½è¸ª...")
                # åŠ è½½åˆå§‹ç‚¹
                initials_path = initials_csv or Path("input/western_pacific_typhoons_superfast.csv")
                initials_df = it_load_initial_points(initials_path)
                # é’ˆå¯¹å½“å‰ NC è¿è¡Œè¿½è¸ª, initialTracker ä¼šä¸ºæ¯ä¸ªé£æš´è¾“å‡ºä¸€ä¸ª CSV
                per_storm_csvs = it_track_file_with_initials(Path(nc_local), initials_df, track_dir)
                if not per_storm_csvs:
                    print("âš ï¸ æ— æœ‰æ•ˆè½¨è¿¹ -> è·³è¿‡ç¯å¢ƒåˆ†æ")
                    if not keep_nc:
                        try:
                            nc_local.unlink(); print("ğŸ§¹ å·²åˆ é™¤NC (æ— è½¨è¿¹)")
                        except Exception: pass
                    skipped += 1
                    continue

                # åˆå¹¶ä¸ºå•ä¸€è½¨è¿¹æ–‡ä»¶, å¢åŠ  particle ä¸ time_idx åˆ—ï¼Œä¾¿äºåç»­æå–
                try:
                    import xarray as _xr
                    ds_times = []
                    with _xr.open_dataset(nc_local) as _ds:
                        ds_times = pd.to_datetime(_ds.time.values) if "time" in _ds.coords else []
                    def _nearest_time_idx(ts: pd.Timestamp) -> int:
                        if len(ds_times) == 0:
                            return 0
                        # ç²¾ç¡®åŒ¹é…ä¼˜å…ˆ
                        try:
                            return int(np.argmin(np.abs(ds_times - ts)))
                        except Exception:
                            return 0
                    parts = []
                    for p in per_storm_csvs:
                        df_i = pd.read_csv(p)
                        # è§£æ storm_id è‡ªæ–‡ä»¶å: track_<storm>_<ncstem>.csv
                        s = Path(p).stem
                        m_id = re.match(r"track_(.+?)_" + re.escape(Path(nc_local).stem) + r"$", s)
                        particle_id = m_id.group(1) if m_id else s.replace("track_", "")
                        df_i["particle"] = particle_id
                        # ç»Ÿä¸€æ—¶é—´å¹¶ç”Ÿæˆ time_idx
                        if "time" in df_i.columns:
                            df_i["time"] = pd.to_datetime(df_i["time"], errors="coerce")
                            df_i["time_idx"] = df_i["time"].apply(lambda t: _nearest_time_idx(t) if pd.notnull(t) else 0)
                        else:
                            # è‹¥ç¼ºå°‘æ—¶é—´, ç”¨é¡ºåºç´¢å¼•ä»£æ›¿
                            df_i["time_idx"] = np.arange(len(df_i))
                            # åˆæˆæ—¶é—´åˆ—(å¯é€‰)
                        parts.append(df_i)
                    tracks_df = pd.concat(parts, ignore_index=True)
                    tracks_df.to_csv(track_csv, index=False)
                    print(f"ğŸ’¾ åˆå¹¶ä¿å­˜è½¨è¿¹: {track_csv.name} (å« {tracks_df['particle'].nunique()} æ¡è·¯å¾„)")
                except Exception as ce:
                    print(f"âŒ åˆå¹¶è½¨è¿¹å¤±è´¥: {ce}")
                    raise
            except Exception as e:
                print(f"âŒ è¿½è¸ªå¤±è´¥: {e}")
                traceback.print_exc()
                if not keep_nc:
                    try:
                        nc_local.unlink(); print("ğŸ§¹ å·²åˆ é™¤NC (è¿½è¸ªå¤±è´¥)")
                    except Exception: pass
                skipped += 1
                continue
        else:
            print("ğŸ—ºï¸  å·²å­˜åœ¨è½¨è¿¹CSV, ç›´æ¥ç¯å¢ƒåˆ†æ")

        # ç¯å¢ƒåˆ†æ
        try:
            extractor = TCEnvironmentalSystemsExtractor(str(nc_local), str(track_csv))
            extractor.analyze_and_export_as_json("final_output")
            processed += 1
        except Exception as e:
            print(f"âŒ ç¯å¢ƒåˆ†æå¤±è´¥: {e}")
        finally:
            if not keep_nc:
                try:
                    nc_local.unlink(); print("ğŸ§¹ å·²åˆ é™¤NCæ–‡ä»¶")
                except Exception as ee:
                    print(f"âš ï¸ åˆ é™¤NCå¤±è´¥: {ee}")

    print("\nğŸ“Š æµå¼å¤„ç†ç»“æœ:")
    print(f"  âœ… å®Œæˆ: {processed}")
    print(f"  â­ï¸ è·³è¿‡: {skipped}")
    print(f"  ğŸ“ è¾“å‡ºç›®å½•: final_output")


def main():
    import argparse, sys, subprocess

    parser = argparse.ArgumentParser(description="ä¸€ä½“åŒ–: ä¸‹è½½->è¿½è¸ª->ç¯å¢ƒåˆ†æ")
    parser.add_argument("--csv", default="output/nc_file_urls.csv", help="å«s3_urlçš„åˆ—è¡¨CSV")
    parser.add_argument("--limit", type=int, default=1, help="é™åˆ¶å¤„ç†å‰Nä¸ªNCæ–‡ä»¶")
    parser.add_argument("--nc", default=None, help="ç›´æ¥æŒ‡å®šå•ä¸ªNCæ–‡ä»¶ (è·³è¿‡ä¸‹è½½ä¸è¿½è¸ª)")
    parser.add_argument("--tracks", default=None, help="ç›´æ¥æŒ‡å®šè½¨è¿¹CSV (è·³è¿‡è¿½è¸ª)\nè‹¥ä¸--ncåŒæ—¶ç»™å‡ºåˆ™åªåšç¯å¢ƒåˆ†æ")
    parser.add_argument("--no-clean", action="store_true", help="åˆ†æåä¸åˆ é™¤NC")
    parser.add_argument("--keep-nc", action="store_true", help="åŒ --no-clean (å…¼å®¹)")
    parser.add_argument("--auto", action="store_true", help="æ— è½¨è¿¹åˆ™è‡ªåŠ¨è¿è¡Œè¿½è¸ª")
    parser.add_argument("--search-range", type=float, default=3.0, help="è¿½è¸ªæœç´¢èŒƒå›´")
    parser.add_argument("--memory", type=int, default=3, help="è¿½è¸ªè®°å¿†æ—¶é—´æ­¥")
    parser.add_argument("--initials", default=str(Path("input")/"western_pacific_typhoons_superfast.csv"), help="initialTracker åˆå§‹ç‚¹CSV")
    parser.add_argument("--batch", action="store_true", help="ä½¿ç”¨æ—§çš„æ‰¹é‡æ¨¡å¼: å…ˆå…¨éƒ¨ä¸‹è½½+è¿½è¸ª, å†ç»Ÿä¸€åšç¯å¢ƒåˆ†æ")
    args = parser.parse_args()

    print("ğŸŒ€ ä¸€ä½“åŒ–çƒ­å¸¦æ°”æ—‹åˆ†ææµç¨‹å¯åŠ¨")
    print("=" * 60)

    nc_file: Path | None = None
    track_file: Path | None = None

    # 1. å•æ–‡ä»¶ç›´é€šæ¨¡å¼ (--nc) æˆ– CSV å¤šæ–‡ä»¶é¡ºåºæµå¼æ¨¡å¼ (é»˜è®¤) / æ—§æ‰¹é‡æ¨¡å¼ (--batch)
    if args.nc:
        nc_file = Path(args.nc)
        if not nc_file.exists():
            print(f"âŒ æŒ‡å®šNCä¸å­˜åœ¨: {nc_file}")
            sys.exit(1)
        target_nc_files = [nc_file]
        print("ğŸ“¦ å•æ–‡ä»¶åˆ†ææ¨¡å¼")
    else:
        if args.batch:
            # æ—§æ‰¹é‡æ¨¡å¼: å…¼å®¹åŸé€»è¾‘
            from trackTC import process_from_csv
            print("â¬‡ï¸ [æ‰¹é‡æ¨¡å¼] å…ˆç»Ÿä¸€ä¸‹è½½/è¿½è¸ªåå†åšç¯å¢ƒåˆ†æ (limit=", args.limit, ")")
            process_from_csv(Path(args.csv), limit=args.limit)
            cache_dir = Path("data/nc_files")
            if not cache_dir.exists():
                print("âŒ æ²¡æœ‰æ‰¾åˆ° data/nc_files ç›®å½•")
                sys.exit(1)
            cached = sorted(cache_dir.glob("*.nc"))
            if not cached:
                print("âŒ æœªå‘ç°ä»»ä½•NCæ–‡ä»¶")
                sys.exit(1)
            target_nc_files = cached[: args.limit] if args.limit is not None else cached
            print(f"ğŸ“¦ å¾…ç¯å¢ƒåˆ†æNCæ•°é‡: {len(target_nc_files)}")
        else:
            # æ–°çš„æµå¼é¡ºåºå¤„ç†: é€æ¡CSV -> ä¸‹è½½ -> è¿½è¸ª -> ç¯å¢ƒåˆ†æ -> (å¯é€‰æ¸…ç†)
            print("ğŸšš å¯ç”¨æµå¼é¡ºåºå¤„ç†: æ¯ä¸ªNCç‹¬ç«‹å®Œæˆ(ä¸‹è½½->è¿½è¸ª->ç¯å¢ƒåˆ†æ->æ¸…ç†)")
            streaming_from_csv(
                csv_path=Path(args.csv),
                limit=args.limit,
                search_range=args.search_range,
                memory=args.memory,
                keep_nc=(args.no_clean or args.keep_nc),
                initials_csv=Path(args.initials) if args.initials else None,
            )
            print("ğŸ¯ æµå¼å¤„ç†å®Œæˆ (æ— éœ€è¿›å…¥æ‰¹é‡åå¤„ç†å¾ªç¯)")
            return

    final_output_dir = Path("final_output")
    final_output_dir.mkdir(exist_ok=True)

    processed = 0
    skipped = 0
    for idx, nc_file in enumerate(target_nc_files, start=1):
        nc_stem = nc_file.stem
        print(f"\n[{idx}/{len(target_nc_files)}] â–¶ï¸ å¤„ç† NC: {nc_file.name}")
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¾“å‡º
        existing = list(final_output_dir.glob(f"{nc_stem}_TC_Analysis_*.json"))
        non_empty = [p for p in existing if p.stat().st_size > 10]
        if non_empty:
            print(f"â­ï¸  å·²å­˜åœ¨åˆ†æç»“æœ ({len(non_empty)}) -> è·³è¿‡ {nc_stem}")
            skipped += 1
            continue

        # å¯»æ‰¾åŒ¹é…çš„è½¨è¿¹æ–‡ä»¶ (ä¼˜å…ˆ forecast_tag åŒ¹é…)
        track_file = None
        if args.tracks:
            t = Path(args.tracks)
            if t.exists():
                track_file = t
        if track_file is None:
            tdir = Path("track_output")
            if tdir.exists():
                forecast_tag_match = re.search(r"(f\d{3}_f\d{3}_\d{2})", nc_stem)
                potential = []
                if forecast_tag_match:
                    tag = forecast_tag_match.group(1)
                    potential = list(tdir.glob(f"tracks_*_{tag}.csv"))
                tracks_all = sorted(tdir.glob("tracks_*.csv"))
                if potential:
                    track_file = potential[0]
                elif tracks_all:
                    # é€€åŒ–é€‰æ‹©: é€‰ç¬¬ä¸€ä¸ª (æç¤ºä¸ç²¾ç¡®)
                    track_file = tracks_all[0]
                    print(f"âš ï¸ æœªç²¾ç¡®åŒ¹é… forecast_tag, ä½¿ç”¨ {track_file.name}")
        if track_file is None:
            if args.auto:
                # ä½¿ç”¨ initialTracker è‡ªåŠ¨ç”Ÿæˆè½¨è¿¹ (åŸºäºåˆå§‹ç‚¹)
                from initialTracker import track_file_with_initials as it_track_file_with_initials
                # å…¼å®¹: initialTracker ä¸­æä¾›çš„æ˜¯ _load_all_pointsï¼Œè¿™é‡Œç”¨åŒååˆ«åå¼•ç”¨
                from initialTracker import _load_all_points as it_load_initial_points
                print("ğŸ”„ ä½¿ç”¨ initialTracker è‡ªåŠ¨è¿½è¸ªå½“å‰NCä»¥ç”Ÿæˆè½¨è¿¹...")
                try:
                    initials_df = it_load_initial_points(Path(args.initials) if args.initials else Path("input/western_pacific_typhoons_superfast.csv"))
                    out_dir = Path("track_output"); out_dir.mkdir(exist_ok=True)
                    per_storm = it_track_file_with_initials(Path(nc_file), initials_df, out_dir)
                    if not per_storm:
                        print("âš ï¸ æ— è½¨è¿¹ -> è·³è¿‡è¯¥NC")
                        skipped += 1
                        continue
                    # åˆå¹¶
                    import xarray as _xr, re as _re
                    ds_times = []
                    with _xr.open_dataset(nc_file) as _ds:
                        ds_times = pd.to_datetime(_ds.time.values) if "time" in _ds.coords else []
                    def _nearest_idx(ts: pd.Timestamp) -> int:
                        if len(ds_times) == 0:
                            return 0
                        return int(np.argmin(np.abs(ds_times - ts)))
                    parts = []
                    for p in per_storm:
                        dfi = pd.read_csv(p)
                        s = Path(p).stem
                        mid = _re.match(r"track_(.+?)_" + _re.escape(nc_stem) + r"$", s)
                        pid = mid.group(1) if mid else s.replace("track_", "")
                        dfi["particle"] = pid
                        if "time" in dfi.columns:
                            dfi["time"] = pd.to_datetime(dfi["time"], errors="coerce")
                            dfi["time_idx"] = dfi["time"].apply(lambda t: _nearest_idx(t) if pd.notnull(t) else 0)
                        else:
                            dfi["time_idx"] = np.arange(len(dfi))
                        parts.append(dfi)
                    tracks_df = pd.concat(parts, ignore_index=True)
                    ts0 = pd.to_datetime(tracks_df.iloc[0]["time"]).strftime("%Y%m%d%H") if "time" in tracks_df.columns and pd.notnull(tracks_df.iloc[0]["time"]) else "T000"
                    track_file = out_dir / f"tracks_auto_{nc_stem}_{ts0}.csv"
                    tracks_df.to_csv(track_file, index=False)
                    print(f"ğŸ’¾ è‡ªåŠ¨è½¨è¿¹æ–‡ä»¶: {track_file.name}")
                except Exception as e:
                    print(f"âŒ è‡ªåŠ¨è¿½è¸ªå¤±è´¥: {e}")
                    skipped += 1
                    continue
            else:
                print("âš ï¸ æœªæ‰¾åˆ°å¯¹åº”è½¨è¿¹ä¸”æœªå¯ç”¨ --auto, è·³è¿‡")
                skipped += 1
                continue

        print(f"âœ… ä½¿ç”¨è½¨è¿¹æ–‡ä»¶: {track_file}")
        try:
            extractor = TCEnvironmentalSystemsExtractor(str(nc_file), str(track_file))
            extractor.analyze_and_export_as_json("final_output")
            processed += 1
        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥ {nc_file.name}: {e}")
            continue

        if not (args.no_clean or args.keep_nc):
            try:
                nc_file.unlink()
                print(f"ğŸ§¹ å·²åˆ é™¤ NC: {nc_file.name}")
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤NCå¤±è´¥: {e}")
        else:
            print("â„¹ï¸ æŒ‰å‚æ•°ä¿ç•™NCæ–‡ä»¶")

    print("\nğŸ‰ å¤šæ–‡ä»¶ç¯å¢ƒåˆ†æå®Œæˆ. ç»Ÿè®¡:")
    print(f"  âœ… å·²åˆ†æ: {processed}")
    print(f"  â­ï¸ è·³è¿‡(å·²æœ‰ç»“æœ/æ— è½¨è¿¹): {skipped}")
    print(f"  ğŸ“¦ æ€»è®¡éå†: {len(target_nc_files)}")
    print("ç»“æœç›®å½•: final_output")


if __name__ == "__main__":
    main()
