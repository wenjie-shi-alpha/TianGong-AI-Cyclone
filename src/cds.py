#!/usr/bin/env python3
"""
CDSæœåŠ¡å™¨ç¯å¢ƒæ°”è±¡ç³»ç»Ÿæå–å™¨
åŸºäºERA5æ•°æ®å’Œå°é£è·¯å¾„æ–‡ä»¶ï¼Œæå–å…³é”®å¤©æ°”ç³»ç»Ÿ
ä¸“ä¸ºCDSæœåŠ¡å™¨ç¯å¢ƒä¼˜åŒ–
"""

import xarray as xr
import numpy as np
import pandas as pd
from datetime import datetime
import json
import cdsapi
import os
import sys
from pathlib import Path
import warnings
import concurrent.futures
import gc
import math

warnings.filterwarnings('ignore')

class CDSEnvironmentExtractor:
    """
    CDSæœåŠ¡å™¨ç¯å¢ƒæ°”è±¡ç³»ç»Ÿæå–å™¨
    """

    def __init__(self, tracks_file, output_dir="./cds_output", cleanup_intermediate=True, max_workers=None, dask_chunks_env="CDS_XR_CHUNKS"):
        """
        åˆå§‹åŒ–æå–å™¨

        Args:
            tracks_file: å°é£è·¯å¾„CSVæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            cleanup_intermediate: æ˜¯å¦åœ¨åˆ†æå®Œæˆåæ¸…ç†ä¸­é—´ERA5æ•°æ®æ–‡ä»¶
            max_workers: å¹¶è¡Œå¤„ç†çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ˆNone=è‡ªåŠ¨ï¼Œ1=ç¦ç”¨å¹¶è¡Œï¼‰
            dask_chunks_env: ä»ç¯å¢ƒå˜é‡è¯»å–xarrayåˆ†å—è®¾ç½®çš„é”®åï¼ˆä¾‹å¦‚ "time:1,latitude:200,longitude:200"ï¼‰
        """
        self.tracks_file = tracks_file
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.cleanup_intermediate = cleanup_intermediate
        self.max_workers = max_workers
        self.dask_chunks_env = dask_chunks_env

        # CDS APIå®¢æˆ·ç«¯
        self._check_cdsapi_config()
        self.cds_client = cdsapi.Client()

        # åŠ è½½å°é£è·¯å¾„æ•°æ®
        self.load_tracks_data()

        # ä¸‹è½½æ–‡ä»¶è®°å½•ï¼Œä¾¿äºåç»­æ¸…ç†
        self._downloaded_files = []

        print("âœ… CDSç¯å¢ƒæå–å™¨åˆå§‹åŒ–å®Œæˆ")

    def _check_cdsapi_config(self):
        """æ£€æŸ¥CDS APIé…ç½®æ˜¯å¦å¯ç”¨ï¼Œå¹¶ç»™å‡ºæç¤ºï¼ˆåœ¨CDS JupyterLabä¸­å°¤ä¸ºé‡è¦ï¼‰"""
        try:
            test_client = cdsapi.Client()
            print("ğŸ› ï¸ CDS APIå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            return True
        except Exception as e:
            print(f"âš ï¸ CDS APIé…ç½®éªŒè¯å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿åœ¨CDS JupyterLabç¯å¢ƒä¸­è¿è¡Œï¼Œæˆ–æ­£ç¡®é…ç½®CDS APIå‡­æ®")
            return False

    def load_tracks_data(self):
        """åŠ è½½å°é£è·¯å¾„æ•°æ®"""
        try:
            self.tracks_df = pd.read_csv(self.tracks_file)
            self.tracks_df['datetime'] = pd.to_datetime(self.tracks_df['datetime'])

            # é‡å‘½ååˆ—ä»¥åŒ¹é…æ ‡å‡†æ ¼å¼
            column_mapping = {
                'latitude': 'lat',
                'longitude': 'lon',
                'datetime': 'time',
                'storm_id': 'particle'
            }

            self.tracks_df = self.tracks_df.rename(columns=column_mapping)

            # æ·»åŠ time_idxåˆ—
            self.tracks_df['time_idx'] = range(len(self.tracks_df))

            print(f"ğŸ“Š åŠ è½½äº† {len(self.tracks_df)} ä¸ªè·¯å¾„ç‚¹")
            print(f"ğŸŒ€ å°é£ID: {self.tracks_df['particle'].unique()}")

        except Exception as e:
            print(f"âŒ åŠ è½½è·¯å¾„æ•°æ®å¤±è´¥: {e}")
            sys.exit(1)

    def download_era5_data(self, start_date, end_date, area=None):
        """
        ä»CDSä¸‹è½½ERA5æ•°æ®

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            area: åŒºåŸŸ [north, west, south, east]
        """
        if area is None:
            # åŸºäºè·¯å¾„æ•°æ®ç¡®å®šåŒºåŸŸ
            lat_min = self.tracks_df['lat'].min() - 10
            lat_max = self.tracks_df['lat'].max() + 10
            lon_min = self.tracks_df['lon'].min() - 10
            lon_max = self.tracks_df['lon'].max() + 10
            area = [lat_max, lon_min, lat_min, lon_max]

        output_file = self.output_dir / f"era5_single_{start_date.replace('-', '')}_{end_date.replace('-', '')}.nc"

        if output_file.exists():
            print(f"ğŸ“ ERA5æ•°æ®å·²å­˜åœ¨: {output_file}")
            self._downloaded_files.append(str(output_file))
            return str(output_file)

        print(f"ğŸ“¥ ä¸‹è½½ERA5æ•°æ®: {start_date} åˆ° {end_date}")

        try:
            date_range = pd.date_range(start=start_date, end=end_date, freq='D')
            
            self.cds_client.retrieve(
                'reanalysis-era5-single-levels',
                {
                    'product_type': 'reanalysis',
                    'format': 'netcdf',
                    'variable': [
                        'mean_sea_level_pressure', '10m_u_component_of_wind',
                        '10m_v_component_of_wind', '2m_temperature',
                        'sea_surface_temperature', 'total_column_water_vapour'
                    ],
                    'year': sorted(list(set(date_range.year))),
                    'month': sorted(list(set(date_range.month))),
                    'day': sorted(list(set(date_range.day))),
                    'time': [
                        '00:00', '06:00', '12:00', '18:00'
                    ],
                    'area': area,
                },
                str(output_file)
            )

            print(f"âœ… ERA5æ•°æ®ä¸‹è½½å®Œæˆ: {output_file}")
            self._downloaded_files.append(str(output_file))
            return str(output_file)

        except Exception as e:
            print(f"âŒ ERA5æ•°æ®ä¸‹è½½å¤±è´¥: {e}")
            return None

    def download_era5_pressure_data(self, start_date, end_date, area=None, levels=("850","500","200")):
        """ä»CDSä¸‹è½½ERA5ç­‰å‹é¢æ•°æ®"""
        if area is None:
            lat_min = self.tracks_df['lat'].min() - 10
            lat_max = self.tracks_df['lat'].max() + 10
            lon_min = self.tracks_df['lon'].min() - 10
            lon_max = self.tracks_df['lon'].max() + 10
            area = [lat_max, lon_min, lat_min, lon_max]

        output_file = self.output_dir / f"era5_pressure_{start_date.replace('-', '')}_{end_date.replace('-', '')}.nc"

        if output_file.exists():
            print(f"ğŸ“ ERA5ç­‰å‹é¢æ•°æ®å·²å­˜åœ¨: {output_file}")
            self._downloaded_files.append(str(output_file))
            return str(output_file)

        print(f"ğŸ“¥ ä¸‹è½½ERA5ç­‰å‹é¢æ•°æ®: {start_date} åˆ° {end_date}")

        try:
            date_range = pd.date_range(start=start_date, end=end_date, freq='D')
            
            self.cds_client.retrieve(
                'reanalysis-era5-pressure-levels',
                {
                    'product_type': 'reanalysis',
                    'format': 'netcdf',
                    'variable': [
                        'u_component_of_wind', 'v_component_of_wind',
                        'geopotential', 'temperature', 'relative_humidity'
                    ],
                    'pressure_level': list(levels),
                    'year': sorted(list(set(date_range.year))),
                    'month': sorted(list(set(date_range.month))),
                    'day': sorted(list(set(date_range.day))),
                    'time': ['00:00', '06:00', '12:00', '18:00'],
                    'area': area,
                },
                str(output_file)
            )

            print(f"âœ… ERA5ç­‰å‹é¢æ•°æ®ä¸‹è½½å®Œæˆ: {output_file}")
            self._downloaded_files.append(str(output_file))
            return str(output_file)

        except Exception as e:
            print(f"âŒ ERA5ç­‰å‹é¢æ•°æ®ä¸‹è½½å¤±è´¥: {e}")
            return None

    def _parse_chunks_from_env(self):
        """ä»ç¯å¢ƒå˜é‡è§£æxarrayåˆ†å—è®¾ç½®"""
        chunks_str = os.environ.get(self.dask_chunks_env, "").strip()
        if not chunks_str:
            return None
        chunks = {}
        try:
            for part in chunks_str.split(','):
                k, v = part.split(':')
                chunks[k.strip()] = int(v.strip())
            return chunks
        except Exception:
            print(f"âš ï¸ æ— æ³•è§£æ {self.dask_chunks_env} ç¯å¢ƒå˜é‡çš„åˆ†å—è®¾ç½®: '{chunks_str}'")
            return None

    def load_era5_data(self, single_file, pressure_file=None):
        """åŠ è½½ERA5æ•°æ®æ–‡ä»¶"""
        try:
            chunks = self._parse_chunks_from_env()
            open_kwargs = {"chunks": chunks} if chunks else {}

            ds_single = xr.open_dataset(single_file, **open_kwargs)
            
            if pressure_file and Path(pressure_file).exists():
                ds_pressure = xr.open_dataset(pressure_file, **open_kwargs)
                self.ds = xr.merge([ds_single, ds_pressure])
            else:
                self.ds = ds_single

            print(f"ğŸ“Š ERA5æ•°æ®åŠ è½½å®Œæˆ: {dict(self.ds.dims)}")
            if 'latitude' in self.ds and 'longitude' in self.ds:
                print(f"ğŸŒ åæ ‡èŒƒå›´: lat {self.ds.latitude.min().values:.1f}Â°-{self.ds.latitude.max().values:.1f}Â°, "
                      f"lon {self.ds.longitude.min().values:.1f}Â°-{self.ds.longitude.max().values:.1f}Â°")
            self._initialize_coordinate_metadata()
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½ERA5æ•°æ®å¤±è´¥: {e}")
            return False

    def _initialize_coordinate_metadata(self):
        """åˆå§‹åŒ–ç»çº¬åº¦ã€æ—¶é—´ç­‰å…ƒæ•°æ®ï¼Œä¾¿äºåç»­ä¸é«˜çº§æå–é€»è¾‘ä¿æŒä¸€è‡´"""
        # çº¬åº¦åæ ‡
        lat_coord = next((name for name in ("latitude", "lat") if name in self.ds.coords), None)
        lon_coord = next((name for name in ("longitude", "lon") if name in self.ds.coords), None)
        if lat_coord is None or lon_coord is None:
            raise ValueError("æ•°æ®é›†ä¸­ç¼ºå°‘çº¬åº¦æˆ–ç»åº¦åæ ‡")

        self._lat_name = lat_coord
        self._lon_name = lon_coord
        self.latitudes = np.asarray(self.ds[self._lat_name].values)
        self.longitudes = np.asarray(self.ds[self._lon_name].values)

        # å¤„ç†ç»åº¦åˆ° [0, 360) åŒºé—´çš„æ ‡å‡†åŒ–å½¢å¼ï¼Œä¾¿äºè·ç¦»åˆ¤æ–­
        self._lon_normalized = self._normalize_lon(self.longitudes)

        # ç»´åº¦é—´è·ï¼ˆåº¦ï¼‰ï¼Œå¦‚åªæœ‰å•ç‚¹åˆ™ä½¿ç”¨1é¿å…é™¤é›¶
        if self.latitudes.size > 1:
            self.lat_spacing = float(np.abs(np.diff(self.latitudes).mean()))
        else:
            self.lat_spacing = 1.0

        if self.longitudes.size > 1:
            sorted_unique_lon = np.sort(np.unique(self.longitudes))
            diffs = np.abs(np.diff(sorted_unique_lon))
            self.lon_spacing = float(diffs[diffs > 0].mean()) if np.any(diffs > 0) else 1.0
        else:
            self.lon_spacing = 1.0

        cos_lat = np.cos(np.deg2rad(self.latitudes))
        self._coslat = cos_lat
        self._coslat_safe = np.where(np.abs(cos_lat) < 1e-6, np.nan, cos_lat)

        # æ—¶é—´è½´
        time_dim = None
        time_coord_name = None
        if 'time' in self.ds.dims:
            time_dim = 'time'
        elif 'valid_time' in self.ds.dims:
            time_dim = 'valid_time'

        if time_dim is None:
            for candidate in ("time", "valid_time"):
                if candidate in self.ds.coords:
                    dims = self.ds[candidate].dims
                    if dims:
                        time_dim = dims[0]
                        time_coord_name = candidate
                        break

        if time_dim is None:
            raise ValueError("æ•°æ®é›†ä¸­ç¼ºå°‘æ—¶é—´ç»´åº¦")

        if time_coord_name is None:
            time_coord_name = time_dim

        self._time_dim = time_dim
        self._time_coord_name = time_coord_name
        time_coord = self.ds[time_coord_name]
        time_values = pd.to_datetime(time_coord.values)
        self._time_values = np.asarray(time_values)

        # ä¿ç•™è¾…åŠ©ç´¢å¼•å‡½æ•°
        def _loc_idx(lat_val: float, lon_val: float):
            lat_idx = int(np.abs(self.latitudes - lat_val).argmin())
            lon_idx = int(np.abs(self.longitudes - lon_val).argmin())
            return lat_idx, lon_idx

        self._loc_idx = _loc_idx

    def extract_environmental_systems(self, time_point, tc_lat, tc_lon):
        """æå–æŒ‡å®šæ—¶é—´ç‚¹çš„ç¯å¢ƒç³»ç»Ÿï¼Œè¾“å‡ºæ ¼å¼ä¸environment_extractorä¿æŒä¸€è‡´"""
        systems = []
        try:
            time_idx, era5_time = self._get_time_index(time_point)
            print(f"ğŸ” å¤„ç†æ—¶é—´ç‚¹: {time_point} (ERA5æ—¶é—´: {era5_time})")

            ds_at_time = self._dataset_at_index(time_idx)

            system_extractors = [
                lambda: self.extract_subtropical_high(time_idx, ds_at_time, tc_lat, tc_lon),
                lambda: self.extract_vertical_shear(time_idx, tc_lat, tc_lon),
                lambda: self.extract_ocean_heat(time_idx, tc_lat, tc_lon),
                lambda: self.extract_low_level_flow(ds_at_time, tc_lat, tc_lon),
                lambda: self.extract_atmospheric_stability(ds_at_time, tc_lat, tc_lon),
            ]

            for extractor in system_extractors:
                system_obj = extractor()
                if system_obj:
                    systems.append(system_obj)

        except Exception as e:
            print(f"âš ï¸ æå–ç¯å¢ƒç³»ç»Ÿå¤±è´¥: {e}")
            systems.append({"system_name": "ExtractionError", "description": str(e)})
        return systems

    def _normalize_lon(self, lon_values):
        arr = np.asarray(lon_values, dtype=np.float64)
        return np.mod(arr + 360.0, 360.0)

    def _lon_distance(self, lon_values, center_lon):
        lon_norm = self._normalize_lon(lon_values)
        center = float(self._normalize_lon(center_lon))
        diff = np.abs(lon_norm - center)
        return np.minimum(diff, 360.0 - diff)

    def _get_time_index(self, target_time):
        if not hasattr(self, "_time_values"):
            raise ValueError("å°šæœªåˆå§‹åŒ–æ—¶é—´è½´ä¿¡æ¯")
        target_ts = pd.Timestamp(target_time)
        target_np = np.datetime64(target_ts.to_datetime64())
        diffs = np.abs(self._time_values - target_np).astype('timedelta64[s]').astype(np.int64)
        idx = int(diffs.argmin())
        era5_time = pd.Timestamp(self._time_values[idx]).to_pydatetime()
        return idx, era5_time

    def _dataset_at_index(self, time_idx):
        selector = {self._time_dim: time_idx} if self._time_dim in self.ds.dims else {}
        ds_at_time = self.ds.isel(**selector)
        if 'expver' in ds_at_time.dims:
            ds_at_time = ds_at_time.isel(expver=0)
        return ds_at_time.squeeze()

    def _get_field_at_time(self, var_name, time_idx):
        if var_name not in self.ds.data_vars:
            return None
        data = self.ds[var_name]
        indexers = {}
        if self._time_dim in data.dims:
            indexers[self._time_dim] = time_idx
        field = data.isel(**indexers)
        if 'expver' in field.dims:
            field = field.isel(expver=0)
        return field.squeeze()

    def _get_data_at_level(self, var_name, level_hpa, time_idx):
        if var_name not in self.ds.data_vars:
            return None
        data = self.ds[var_name]
        indexers = {}
        if self._time_dim in data.dims:
            indexers[self._time_dim] = time_idx
        level_dim = next((dim for dim in ("level", "isobaricInhPa", "pressure") if dim in data.dims), None)
        if level_dim is None:
            field = data.isel(**indexers)
        else:
            if level_dim in data.coords:
                level_values = data[level_dim].values
            elif level_dim in self.ds.coords:
                level_values = self.ds[level_dim].values
            else:
                level_values = np.arange(data.sizes[level_dim])
            level_idx = int(np.abs(level_values - level_hpa).argmin())
            indexers[level_dim] = level_idx
            field = data.isel(**indexers)
        if 'expver' in field.dims:
            field = field.isel(expver=0)
        result = field.squeeze()
        return result.values if hasattr(result, 'values') else np.asarray(result)

    def _get_sst_field(self, time_idx):
        for var_name in ("sst", "ts"):
            field = self._get_field_at_time(var_name, time_idx)
            if field is not None:
                values = field.values if hasattr(field, 'values') else np.asarray(field)
                if np.nanmean(values) > 200:
                    values = values - 273.15
                return values
        for var_name in ("t2m", "t2"):
            field = self._get_field_at_time(var_name, time_idx)
            if field is not None:
                values = field.values if hasattr(field, 'values') else np.asarray(field)
                if np.nanmean(values) > 200:
                    values = values - 273.15
                return values
        return None

    def _create_region_mask(self, center_lat, center_lon, radius_deg):
        lat_mask = (self.latitudes >= center_lat - radius_deg) & (self.latitudes <= center_lat + radius_deg)
        lon_mask = self._lon_distance(self.longitudes, center_lon) <= radius_deg
        return np.outer(lat_mask, lon_mask)

    def _haversine_distance(self, lat1, lon1, lat2, lon2):
        lat1_rad = np.radians(lat1)
        lon1_rad = np.radians(lon1)
        lat2_rad = np.radians(lat2)
        lon2_rad = np.radians(lon2)
        dlat = lat2_rad - lat1_rad
        dlon = lon2_rad - lon1_rad
        a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0) ** 2
        c = 2.0 * np.arcsin(np.sqrt(a))
        return 6371.0 * c

    def _calculate_distance(self, lat1, lon1, lat2, lon2):
        return float(self._haversine_distance(lat1, lon1, lat2, lon2))

    def _calculate_bearing(self, lat1, lon1, lat2, lon2):
        lat1_rad = math.radians(lat1)
        lat2_rad = math.radians(lat2)
        dlon = math.radians(lon2 - lon1)
        y = math.sin(dlon) * math.cos(lat2_rad)
        x = math.cos(lat1_rad) * math.sin(lat2_rad) - math.sin(lat1_rad) * math.cos(lat2_rad) * math.cos(dlon)
        bearing = (math.degrees(math.atan2(y, x)) + 360.0) % 360.0
        return bearing

    def _bearing_to_desc(self, bearing):
        dirs = [
            "åŒ—",
            "ä¸œåŒ—ååŒ—",
            "ä¸œåŒ—",
            "ä¸œåŒ—åä¸œ",
            "ä¸œ",
            "ä¸œå—åä¸œ",
            "ä¸œå—",
            "ä¸œå—åå—",
            "å—",
            "è¥¿å—åå—",
            "è¥¿å—",
            "è¥¿å—åè¥¿",
            "è¥¿",
            "è¥¿åŒ—åè¥¿",
            "è¥¿åŒ—",
            "è¥¿åŒ—ååŒ—",
        ]
        wind_dirs = [
            "ååŒ—é£",
            "ä¸œåŒ—ååŒ—é£",
            "ä¸œåŒ—é£",
            "ä¸œåŒ—åä¸œé£",
            "åä¸œé£",
            "ä¸œå—åä¸œé£",
            "ä¸œå—é£",
            "ä¸œå—åå—é£",
            "åå—é£",
            "è¥¿å—åå—é£",
            "è¥¿å—é£",
            "è¥¿å—åè¥¿é£",
            "åè¥¿é£",
            "è¥¿åŒ—åè¥¿é£",
            "è¥¿åŒ—é£",
            "è¥¿åŒ—ååŒ—é£",
        ]
        index = int(round(bearing / 22.5)) % 16
        return wind_dirs[index], f"{dirs[index]}æ–¹å‘"

    def _get_vector_coords(self, lat, lon, u, v, scale=0.1):
        factor = scale * 0.009
        end_lat = lat + v * factor
        cos_lat = math.cos(math.radians(lat))
        if abs(cos_lat) < 1e-6:
            cos_lat = 1e-6
        end_lon = lon + u * factor / cos_lat
        return {
            "start": {"lat": round(lat, 2), "lon": round(lon, 2)},
            "end": {"lat": round(end_lat, 2), "lon": round(end_lon, 2)},
        }

    def extract_subtropical_high(self, time_idx, ds_at_time, tc_lat, tc_lon):
        """æå–å‰¯çƒ­å¸¦é«˜å‹ç³»ç»Ÿï¼Œä¸environment_extractorè¾“å‡ºè¯­ä¹‰ä¿æŒä¸€è‡´"""
        try:
            z500 = self._get_data_at_level("z", 500, time_idx)
            field_source = "z500"
            if z500 is not None:
                field_values = np.asarray(z500, dtype=float)
                if np.nanmean(field_values) > 10000:
                    field_values = field_values / 9.80665  # è½¬æ¢ä¸ºgpm
                threshold = 5880
                unit = "gpm"
            else:
                field_source = "msl"
                msl = self._get_field_at_time("msl", time_idx)
                if msl is None:
                    return None
                field_values = (msl.values if hasattr(msl, "values") else np.asarray(msl)) / 100.0
                threshold = 1020
                unit = "hPa"

            if not np.any(np.isfinite(field_values)):
                return None

            mask = np.isfinite(field_values)
            if field_source == "z500":
                mask &= field_values >= threshold
            else:
                mask &= field_values >= threshold

            if np.any(mask):
                candidate_idx = np.argwhere(mask)
                candidate_lats = self.latitudes[candidate_idx[:, 0]]
                candidate_lons = self.longitudes[candidate_idx[:, 1]]
                distances = self._haversine_distance(candidate_lats, candidate_lons, tc_lat, tc_lon)
                if np.all(np.isnan(distances)):
                    target_idx = np.unravel_index(np.nanargmax(field_values), field_values.shape)
                else:
                    best = int(np.nanargmin(distances))
                    target_idx = tuple(candidate_idx[best])
            else:
                target_idx = np.unravel_index(np.nanargmax(field_values), field_values.shape)

            high_lat = float(self.latitudes[target_idx[0]])
            high_lon = float(self.longitudes[target_idx[1]])
            intensity_val = float(field_values[target_idx])

            if field_source == "z500":
                if intensity_val > 5900:
                    level = "å¼º"
                elif intensity_val > 5880:
                    level = "ä¸­ç­‰"
                else:
                    level = "å¼±"
            else:
                if intensity_val >= 1025:
                    level = "å¼º"
                elif intensity_val >= 1020:
                    level = "ä¸­ç­‰"
                else:
                    level = "å¼±"

            bearing = self._calculate_bearing(tc_lat, tc_lon, high_lat, high_lon)
            _, rel_dir_text = self._bearing_to_desc(bearing)
            distance = self._calculate_distance(tc_lat, tc_lon, high_lat, high_lon)

            # å¼•å¯¼æ°”æµä¼°ç®—
            gy, gx = np.gradient(field_values)
            denom_lat = self.lat_spacing * 111000.0 if self.lat_spacing else 1.0
            denom_lon = self.lon_spacing * 111000.0 if self.lon_spacing else 1.0
            lat_idx, lon_idx = self._loc_idx(tc_lat, tc_lon)
            coslat_val = self._coslat_safe[lat_idx] if np.isfinite(self._coslat_safe[lat_idx]) else math.cos(math.radians(tc_lat))
            dx_val = gx[lat_idx, lon_idx] / (denom_lon * coslat_val if coslat_val else denom_lon)
            dy_val = gy[lat_idx, lon_idx] / denom_lat
            u_steering = -dx_val / (9.8 * 1e-5)
            v_steering = dy_val / (9.8 * 1e-5)
            steering_speed = float(np.sqrt(u_steering**2 + v_steering**2))
            steering_direction = (np.degrees(np.arctan2(u_steering, v_steering)) + 180.0) % 360.0
            wind_desc, steering_dir_text = self._bearing_to_desc(steering_direction)

            description = (
                f"ä¸€ä¸ªå¼ºåº¦ä¸ºâ€œ{level}â€çš„å‰¯çƒ­å¸¦é«˜å‹ç³»ç»Ÿä½äºå°é£çš„{rel_dir_text}ï¼Œ"
                f"æ ¸å¿ƒå¼ºåº¦çº¦ä¸º{intensity_val:.0f}{unit}ï¼Œä¸ºå°é£æä¾›æ¥è‡ª{wind_desc}ã€æ–¹å‘"
                f"çº¦{steering_direction:.0f}Â°ã€é€Ÿåº¦{steering_speed:.1f}m/sçš„å¼•å¯¼æ°”æµã€‚"
            )

            shape_info = {
                "description": "åŸºäºé˜ˆå€¼è¯†åˆ«çš„é«˜å‹æ§åˆ¶åŒº",
                "field": field_source,
                "threshold": threshold,
            }

            return {
                "system_name": "SubtropicalHigh",
                "description": description,
                "position": {
                    "description": "å‰¯çƒ­å¸¦é«˜å‹ä¸­å¿ƒ",
                    "center_of_mass": {"lat": round(high_lat, 2), "lon": round(high_lon, 2)},
                    "relative_to_tc": {
                        "direction": rel_dir_text,
                        "bearing_deg": round(bearing, 1),
                        "distance_km": round(distance, 1),
                    },
                },
                "intensity": {"value": round(intensity_val, 1), "unit": unit, "level": level},
                "shape": shape_info,
                "properties": {
                    "influence": "ä¸»å¯¼å°é£æœªæ¥è·¯å¾„",
                    "steering_flow": {
                        "speed_mps": round(steering_speed, 2),
                        "direction_deg": round(steering_direction, 1),
                        "vector_mps": {"u": round(float(u_steering), 2), "v": round(float(v_steering), 2)},
                        "wind_desc": wind_desc,
                    },
                },
            }
        except Exception as e:
            print(f"âš ï¸ æå–å‰¯çƒ­å¸¦é«˜å‹å¤±è´¥: {e}")
            return None

    def extract_ocean_heat(self, time_idx, tc_lat, tc_lon, radius_deg=2.0):
        """æå–æµ·æ´‹çƒ­å«é‡ï¼Œä¸environment_extractorçš„çƒ­åŠ›åˆ¤æ–­ä¿æŒä¸€è‡´"""
        try:
            sst_values = self._get_sst_field(time_idx)
            if sst_values is None:
                return None

            region_mask = self._create_region_mask(tc_lat, tc_lon, radius_deg)
            if not np.any(region_mask):
                return None

            warm_region = np.where(region_mask, sst_values, np.nan)
            mean_temp = float(np.nanmean(warm_region))
            if not np.isfinite(mean_temp):
                return None

            if mean_temp > 29:
                level, impact = "æé«˜", "ä¸ºçˆ†å‘æ€§å¢å¼ºæä¾›é¡¶çº§èƒ½é‡"
            elif mean_temp > 28:
                level, impact = "é«˜", "éå¸¸æœ‰åˆ©äºåŠ å¼º"
            elif mean_temp > 26.5:
                level, impact = "ä¸­ç­‰", "è¶³ä»¥ç»´æŒå¼ºåº¦"
            else:
                level, impact = "ä½", "èƒ½é‡ä¾›åº”ä¸è¶³"

            desc = (
                f"å°é£ä¸‹æ–¹åŠå¾„çº¦{radius_deg}Â°çš„æµ·åŸŸå¹³å‡æµ·è¡¨æ¸©åº¦ä¸º{mean_temp:.1f}Â°Cï¼Œ"
                f"æµ·æ´‹çƒ­å«é‡ç­‰çº§ä¸ºâ€œ{level}â€ï¼Œ{impact}ã€‚"
            )

            cell_lat_km = self.lat_spacing * 111.0 if self.lat_spacing else 0.0
            cell_lon_km = (self.lon_spacing * 111.0 * math.cos(math.radians(tc_lat))) if self.lon_spacing else 0.0
            approx_area = float(region_mask.astype(float).sum() * abs(cell_lat_km) * abs(cell_lon_km))

            shape_info = {
                "description": "å°é£é™„è¿‘æš–æ°´è¦†ç›–åŒº",
                "radius_deg": radius_deg,
            }
            if approx_area > 0:
                shape_info["approx_area_km2"] = round(approx_area, 0)

            return {
                "system_name": "OceanHeatContent",
                "description": desc,
                "position": {
                    "description": f"å°é£ä¸­å¿ƒå‘¨å›´{radius_deg}Â°åŠå¾„å†…çš„æµ·åŸŸ",
                    "lat": round(tc_lat, 2),
                    "lon": round(tc_lon, 2),
                },
                "intensity": {"value": round(mean_temp, 2), "unit": "Â°C", "level": level},
                "shape": shape_info,
                "properties": {"impact": impact, "warm_water_support": mean_temp > 26.5},
            }
        except Exception as e:
            print(f"âš ï¸ æå–æµ·æ´‹çƒ­å«é‡å¤±è´¥: {e}")
            return None

    def extract_low_level_flow(self, ds_at_time, tc_lat, tc_lon):
        """æå–ä½å±‚(10m)é£åœºï¼Œä¿æŒä¸ä¸»æµç¨‹ç›¸åŒçš„ç»“æ„"""
        try:
            if "u10" not in ds_at_time.data_vars or "v10" not in ds_at_time.data_vars:
                return None

            u10 = float(ds_at_time.u10.interp(latitude=tc_lat, longitude=tc_lon, method="linear").values)
            v10 = float(ds_at_time.v10.interp(latitude=tc_lat, longitude=tc_lon, method="linear").values)

            mean_speed = float(np.sqrt(u10**2 + v10**2))
            mean_direction = (np.degrees(np.arctan2(u10, v10)) + 360.0) % 360.0

            if mean_speed >= 15:
                wind_level = "å¼º"
            elif mean_speed >= 10:
                wind_level = "ä¸­ç­‰"
            else:
                wind_level = "å¼±"

            wind_desc, dir_text = self._bearing_to_desc(mean_direction)
            desc = (
                f"è¿‘åœ°å±‚å­˜åœ¨{wind_level}ä½å±‚é£åœºï¼Œé£é€Ÿçº¦{mean_speed:.1f}m/sï¼Œ"
                f"ä¸»å¯¼é£å‘ä¸º{wind_desc} (çº¦{mean_direction:.0f}Â°)ã€‚"
            )

            return {
                "system_name": "LowLevelFlow",
                "description": desc,
                "position": {"lat": round(tc_lat, 2), "lon": round(tc_lon, 2)},
                "intensity": {
                    "speed": round(mean_speed, 2),
                    "direction_deg": round(mean_direction, 1),
                    "unit": "m/s",
                    "level": wind_level,
                    "vector": {"u": round(u10, 2), "v": round(v10, 2)},
                },
                "properties": {"direction_text": dir_text},
            }
        except Exception as e:
            print(f"âš ï¸ æå–ä½å±‚é£åœºå¤±è´¥: {e}")
            return None

    def extract_atmospheric_stability(self, ds_at_time, tc_lat, tc_lon):
        """æå–å¤§æ°”ç¨³å®šæ€§ï¼Œæä¾›ä¸å…¶ä»–ç³»ç»Ÿä¸€è‡´çš„æ•°æ®ç»“æ„"""
        try:
            if "t2m" not in ds_at_time.data_vars:
                return None

            t2m = ds_at_time.t2m
            if np.nanmean(t2m.values) > 200:
                t2m = t2m - 273.15
            point_t2m = float(t2m.interp(latitude=tc_lat, longitude=tc_lon, method="linear").values)
            if point_t2m > 28:
                stability = "ä¸ç¨³å®š"
            elif point_t2m > 24:
                stability = "ä¸­ç­‰"
            else:
                stability = "è¾ƒç¨³å®š"

            desc = f"è¿‘åœ°è¡¨æ¸©åº¦çº¦{point_t2m:.1f}Â°Cï¼Œä½å±‚å¤§æ°”{stability}ã€‚"

            return {
                "system_name": "AtmosphericStability",
                "description": desc,
                "position": {"lat": round(tc_lat, 2), "lon": round(tc_lon, 2)},
                "intensity": {"surface_temp": round(point_t2m, 2), "unit": "Â°C"},
                "properties": {"stability_level": stability},
            }
        except Exception as e:
            print(f"âš ï¸ æå–å¤§æ°”ç¨³å®šæ€§å¤±è´¥: {e}")
            return None

    def extract_vertical_shear(self, time_idx, tc_lat, tc_lon):
        """æå–å‚ç›´é£åˆ‡å˜ï¼Œå¤ç”¨ä¸environment_extractorä¸€è‡´çš„é˜ˆå€¼å’Œæè¿°"""
        try:
            u200 = self._get_data_at_level("u", 200, time_idx)
            v200 = self._get_data_at_level("v", 200, time_idx)
            u850 = self._get_data_at_level("u", 850, time_idx)
            v850 = self._get_data_at_level("v", 850, time_idx)
            if any(x is None for x in (u200, v200, u850, v850)):
                return None

            lat_idx, lon_idx = self._loc_idx(tc_lat, tc_lon)
            shear_u = float(u200[lat_idx, lon_idx] - u850[lat_idx, lon_idx])
            shear_v = float(v200[lat_idx, lon_idx] - v850[lat_idx, lon_idx])
            shear_mag = float(np.sqrt(shear_u**2 + shear_v**2))

            if shear_mag < 5:
                level, impact = "å¼±", "éå¸¸æœ‰åˆ©äºå‘å±•"
            elif shear_mag < 10:
                level, impact = "ä¸­ç­‰", "åŸºæœ¬æœ‰åˆ©å‘å±•"
            else:
                level, impact = "å¼º", "æ˜¾è‘—æŠ‘åˆ¶å‘å±•"

            direction_from = (np.degrees(np.arctan2(shear_u, shear_v)) + 180.0) % 360.0
            wind_desc, dir_text = self._bearing_to_desc(direction_from)

            desc = (
                f"å°é£æ ¸å¿ƒåŒºæ­£å—åˆ°æ¥è‡ª{wind_desc}æ–¹å‘ã€å¼ºåº¦ä¸ºâ€œ{level}â€çš„å‚ç›´é£åˆ‡å˜å½±å“ï¼Œ"
                f"å½“å‰é£åˆ‡å˜ç¯å¢ƒå¯¹å°é£çš„å‘å±•{impact.split(' ')[-1]}ã€‚"
            )

            vector_coords = self._get_vector_coords(tc_lat, tc_lon, shear_u, shear_v)

            return {
                "system_name": "VerticalWindShear",
                "description": desc,
                "position": {
                    "description": "åœ¨å°é£ä¸­å¿ƒç‚¹è®¡ç®—çš„200-850hPaé£çŸ¢é‡å·®",
                    "lat": round(tc_lat, 2),
                    "lon": round(tc_lon, 2),
                },
                "intensity": {"value": round(shear_mag, 2), "unit": "m/s", "level": level},
                "shape": {"description": f"æ¥è‡ª{wind_desc}çš„åˆ‡å˜çŸ¢é‡", "vector_coordinates": vector_coords},
                "properties": {
                    "direction_from_deg": round(direction_from, 1),
                    "impact": impact,
                    "shear_vector_mps": {"u": round(shear_u, 2), "v": round(shear_v, 2)},
                },
            }
        except Exception as e:
            print(f"âš ï¸ æå–å‚ç›´é£åˆ‡å˜å¤±è´¥: {e}")
            return None
            
    # å…¼å®¹æ—§æ¥å£çš„å ä½ç¬¦ï¼Œä¿ç•™åç§°ä»¥é¿å…æ½œåœ¨å¤–éƒ¨è°ƒç”¨
    def _find_nearest_grid(self, lat, lon):
        return self._loc_idx(lat, lon)

    def process_all_tracks(self):
        """
        æŒ‰æœˆä¸‹è½½ã€å¤„ç†ã€ä¿å­˜å’Œæ¸…ç†æ•°æ®ï¼Œæ”¯æŒå¹¶è¡Œè®¡ç®—ã€‚
        """
        # æŒ‰å¹´æœˆåˆ†ç»„
        self.tracks_df['year_month'] = self.tracks_df['time'].dt.to_period('M')
        unique_months = sorted(self.tracks_df['year_month'].unique())
        print(f"ğŸ—“ï¸ æ‰¾åˆ° {len(unique_months)} ä¸ªéœ€è¦å¤„ç†çš„æœˆä»½: {[str(m) for m in unique_months]}")

        saved_files = []
        
        for month in unique_months:
            print(f"\n{'='*25} å¼€å§‹å¤„ç†æœˆä»½: {month} {'='*25}")
            month_tracks_df = self.tracks_df[self.tracks_df['year_month'] == month]
            start_date = month_tracks_df['time'].min().strftime('%Y-%m-%d')
            end_date = month_tracks_df['time'].max().strftime('%Y-%m-%d')
            print(f"ğŸ“… è¯¥æœˆæ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}ï¼Œå…± {len(month_tracks_df)} ä¸ªè·¯å¾„ç‚¹")
            
            single_file = self.download_era5_data(start_date, end_date)
            pressure_file = self.download_era5_pressure_data(start_date, end_date)
            
            if not single_file:
                print(f"âŒ æ— æ³•è·å– {month} çš„å•å±‚æ•°æ®ï¼Œè·³è¿‡æ­¤æœˆä»½")
                continue

            if not self.load_era5_data(single_file, pressure_file):
                print(f"âŒ æ— æ³•åŠ è½½ {month} çš„æ•°æ®ï¼Œè·³è¿‡æ­¤æœˆä»½")
                if self.cleanup_intermediate: self._cleanup_intermediate_files([single_file, pressure_file])
                continue

            # å®šä¹‰ç”¨äºå¹¶è¡Œå¤„ç†çš„å•ç‚¹å¤„ç†å‡½æ•°
            def _process_row(args):
                idx, track_point = args
                time_point = track_point['time']
                tc_lat, tc_lon = track_point['lat'], track_point['lon']
                print(f"ğŸ”„ å¤„ç†è·¯å¾„ç‚¹ {track_point['time_idx']+1}/{len(self.tracks_df)}: {time_point.strftime('%Y-%m-%d %H:%M')}")
                systems = self.extract_environmental_systems(time_point, tc_lat, tc_lon)
                return {
                    "time": time_point.isoformat(), "time_idx": int(track_point['time_idx']),
                    "tc_position": {"lat": tc_lat, "lon": tc_lon},
                    "tc_intensity": {"max_wind": track_point.get('max_wind_wmo', None), "min_pressure": track_point.get('min_pressure_wmo', None)},
                    "environmental_systems": systems
                }

            # å¹¶è¡Œæˆ–ä¸²è¡Œå¤„ç†å½“å‰æœˆä»½çš„è·¯å¾„ç‚¹
            iterable = list(month_tracks_df.iterrows())
            processed_this_month = []
            if self.max_workers is None or self.max_workers > 1:
                workers = self.max_workers or min(8, (os.cpu_count() or 1) + 4)
                print(f"âš™ï¸ ä½¿ç”¨ {workers} ä¸ªå·¥ä½œçº¿ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†...")
                with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as ex:
                    processed_this_month = list(ex.map(_process_row, iterable))
            else:
                print("âš™ï¸ æ­£åœ¨è¿›è¡Œä¸²è¡Œå¤„ç†...")
                processed_this_month = [_process_row(item) for item in iterable]

            # *** æ–°å¢ï¼šä¸ºå½“å‰æœˆä»½åˆ›å»ºå¹¶ä¿å­˜ç»“æœ ***
            if processed_this_month:
                monthly_results = {
                    "metadata": {
                        "extraction_time": datetime.now().isoformat(),
                        "tracks_file": str(self.tracks_file),
                        "total_points_in_month": len(processed_this_month),
                        "month_processed": str(month),
                        "data_source": "ERA5_reanalysis",
                        "processing_mode": "CDS_server_monthly_save"
                    },
                    "environmental_analysis": sorted(processed_this_month, key=lambda x: x['time_idx'])
                }
                
                monthly_output_file = self.output_dir / f"cds_environment_analysis_{month}.json"
                saved_path = self.save_results(monthly_results, output_file=monthly_output_file)
                if saved_path:
                    saved_files.append(saved_path)
            
            # æ¸…ç†å½“æœˆä¸‹è½½çš„æ–‡ä»¶
            if self.cleanup_intermediate:
                print(f"ğŸ§¹ æ­£åœ¨æ¸…ç† {month} çš„ä¸­é—´æ–‡ä»¶...")
                self._cleanup_intermediate_files([single_file, pressure_file])

        print(f"\nâœ… æ‰€æœ‰æœˆä»½å¤„ç†å®Œæ¯•ã€‚")
        return saved_files

    def save_results(self, results, output_file=None):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = self.output_dir / f"cds_environment_analysis_{timestamp}.json"
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {Path(output_file).stat().st_size / 1024:.1f} KB")
            return str(output_file)
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return None

    def _cleanup_intermediate_files(self, files_to_delete):
        """å…³é—­æ•°æ®é›†å¹¶åˆ é™¤æŒ‡å®šçš„ERA5ä¸´æ—¶æ–‡ä»¶ä»¥é‡Šæ”¾ç£ç›˜ç©ºé—´"""
        try:
            if hasattr(self, 'ds') and self.ds is not None:
                self.ds.close()
                self.ds = None
            gc.collect()

            removed_count = 0
            for f in files_to_delete:
                if f and Path(f).exists():
                    try:
                        Path(f).unlink()
                        removed_count += 1
                    except Exception as e:
                        print(f"âš ï¸ æ— æ³•åˆ é™¤æ–‡ä»¶ {f}: {e}")
            if removed_count > 0:
                print(f"ğŸ§¹ æˆåŠŸæ¸…ç† {removed_count} ä¸ªä¸­é—´æ•°æ®æ–‡ä»¶ã€‚")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ä¸­é—´æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    is_jupyter = 'ipykernel' in sys.modules

    if is_jupyter:
        print("ğŸŒ€ æ£€æµ‹åˆ° Jupyter ç¯å¢ƒï¼Œä½¿ç”¨é»˜è®¤å‚æ•°è¿è¡Œ")
        args = type('Args', (), {
            'tracks': 'western_pacific_typhoons_superfast.csv',
            'output': './cds_output',
            'max_points': None,
            'no_clean': False,
            'workers': None
        })()
    else:
        parser = argparse.ArgumentParser(description='CDSæœåŠ¡å™¨ç¯å¢ƒæ°”è±¡ç³»ç»Ÿæå–å™¨')
        parser.add_argument('--tracks', default='western_pacific_typhoons_superfast.csv', help='å°é£è·¯å¾„CSVæ–‡ä»¶è·¯å¾„')
        parser.add_argument('--output', default='./cds_output', help='è¾“å‡ºç›®å½•')
        parser.add_argument('--max-points', type=int, default=None, help='æœ€å¤§å¤„ç†è·¯å¾„ç‚¹æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
        parser.add_argument('--no-clean', action='store_true', help='ä¿ç•™ä¸­é—´ERA5æ•°æ®æ–‡ä»¶')
        parser.add_argument('--workers', type=int, default=None, help='å¹¶è¡Œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤è‡ªåŠ¨ï¼Œ1è¡¨ç¤ºç¦ç”¨å¹¶è¡Œï¼‰')
        args = parser.parse_args()

    print("ğŸŒ€ CDSç¯å¢ƒæ°”è±¡ç³»ç»Ÿæå–å™¨")
    print("=" * 50)
    print(f"ğŸ“ è·¯å¾„æ–‡ä»¶: {args.tracks}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output}")

    extractor = CDSEnvironmentExtractor(args.tracks, args.output, cleanup_intermediate=not args.no_clean, max_workers=args.workers)

    if args.max_points:
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: ä»…å¤„ç†å‰ {args.max_points} ä¸ªè·¯å¾„ç‚¹")
        extractor.tracks_df = extractor.tracks_df.head(args.max_points)

    # process_all_tracksç°åœ¨å¤„ç†æ‰€æœ‰äº‹æƒ…ï¼ŒåŒ…æ‹¬ä¿å­˜
    saved_file_list = extractor.process_all_tracks()

    if saved_file_list:
        print(f"\nâœ… å¤„ç†å®Œæˆï¼å…±ç”Ÿæˆ {len(saved_file_list)} ä¸ªæœˆåº¦ç»“æœæ–‡ä»¶:")
        for file_path in saved_file_list:
            print(f"  -> {file_path}")
    else:
        print("\nâŒ å¤„ç†å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆä»»ä½•ç»“æœæ–‡ä»¶ã€‚")
        sys.exit(1)


if __name__ == "__main__":
    main()
