#!/usr/bin/env python3
"""
CDSæœåŠ¡å™¨ç¯å¢ƒæ°”è±¡ç³»ç»Ÿæå–å™¨
åŸºäºERA5æ•°æ®å’Œå°é£è·¯å¾„æ–‡ä»¶ï¼Œæå–å…³é”®å¤©æ°”ç³»ç»Ÿ
ä¸“ä¸ºCDSæœåŠ¡å™¨ç¯å¢ƒä¼˜åŒ–
"""

import xarray as xr
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import json
import cdsapi
import os
import sys
from pathlib import Path
import warnings
import concurrent.futures
import gc

warnings.filterwarnings('ignore')

class CDSEnvironmentExtractor:
    """
    CDSæœåŠ¡å™¨ç¯å¢ƒæ°”è±¡ç³»ç»Ÿæå–å™¨
    """

    def __init__(self, tracks_file, output_dir="./cds_output", cleanup_intermediate=True, max_workers=None, dask_chunks_env="CDS_XR_CHUNKS"):
        """
        åˆå§‹åŒ–æå–å™¨

        Args:
            tracks_file: å°é£è·¯å¾„CSVæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            cleanup_intermediate: æ˜¯å¦åœ¨åˆ†æå®Œæˆåæ¸…ç†ä¸­é—´ERA5æ•°æ®æ–‡ä»¶
            max_workers: å¹¶è¡Œå¤„ç†çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ˆNone=è‡ªåŠ¨ï¼Œ1=ç¦ç”¨å¹¶è¡Œï¼‰
            dask_chunks_env: ä»ç¯å¢ƒå˜é‡è¯»å–xarrayåˆ†å—è®¾ç½®çš„é”®åï¼ˆä¾‹å¦‚ "time:1,latitude:200,longitude:200"ï¼‰
        """
        self.tracks_file = tracks_file
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.cleanup_intermediate = cleanup_intermediate
        self.max_workers = max_workers
        self.dask_chunks_env = dask_chunks_env

        # CDS APIå®¢æˆ·ç«¯
        self._check_cdsapi_config()
        self.cds_client = cdsapi.Client()

        # åŠ è½½å°é£è·¯å¾„æ•°æ®
        self.load_tracks_data()

        # ä¸‹è½½æ–‡ä»¶è®°å½•ï¼Œä¾¿äºåç»­æ¸…ç†
        self._downloaded_files = []

        print("âœ… CDSç¯å¢ƒæå–å™¨åˆå§‹åŒ–å®Œæˆ")

    def _check_cdsapi_config(self):
        """æ£€æŸ¥CDS APIé…ç½®æ˜¯å¦å¯ç”¨ï¼Œå¹¶ç»™å‡ºæç¤ºï¼ˆåœ¨CDS JupyterLabä¸­å°¤ä¸ºé‡è¦ï¼‰"""
        try:
            test_client = cdsapi.Client()
            print("ğŸ› ï¸ CDS APIå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            return True
        except Exception as e:
            print(f"âš ï¸ CDS APIé…ç½®éªŒè¯å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿åœ¨CDS JupyterLabç¯å¢ƒä¸­è¿è¡Œï¼Œæˆ–æ­£ç¡®é…ç½®CDS APIå‡­æ®")
            return False

    def load_tracks_data(self):
        """åŠ è½½å°é£è·¯å¾„æ•°æ®"""
        try:
            self.tracks_df = pd.read_csv(self.tracks_file)
            self.tracks_df['datetime'] = pd.to_datetime(self.tracks_df['datetime'])

            # é‡å‘½ååˆ—ä»¥åŒ¹é…æ ‡å‡†æ ¼å¼
            column_mapping = {
                'latitude': 'lat',
                'longitude': 'lon',
                'datetime': 'time',
                'storm_id': 'particle'
            }

            self.tracks_df = self.tracks_df.rename(columns=column_mapping)

            # æ·»åŠ time_idxåˆ—
            self.tracks_df['time_idx'] = range(len(self.tracks_df))

            print(f"ğŸ“Š åŠ è½½äº† {len(self.tracks_df)} ä¸ªè·¯å¾„ç‚¹")
            print(f"ğŸŒ€ å°é£ID: {self.tracks_df['particle'].unique()}")

        except Exception as e:
            print(f"âŒ åŠ è½½è·¯å¾„æ•°æ®å¤±è´¥: {e}")
            sys.exit(1)

    def download_era5_data(self, start_date, end_date, area=None):
        """
        ä»CDSä¸‹è½½ERA5æ•°æ®

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            area: åŒºåŸŸ [north, west, south, east]
        """
        if area is None:
            # åŸºäºè·¯å¾„æ•°æ®ç¡®å®šåŒºåŸŸ
            lat_min = self.tracks_df['lat'].min() - 10
            lat_max = self.tracks_df['lat'].max() + 10
            lon_min = self.tracks_df['lon'].min() - 10
            lon_max = self.tracks_df['lon'].max() + 10
            area = [lat_max, lon_min, lat_min, lon_max]

        output_file = self.output_dir / f"era5_single_{start_date.replace('-', '')}_{end_date.replace('-', '')}.nc"

        if output_file.exists():
            print(f"ğŸ“ ERA5æ•°æ®å·²å­˜åœ¨: {output_file}")
            self._downloaded_files.append(str(output_file))
            return str(output_file)

        print(f"ğŸ“¥ ä¸‹è½½ERA5æ•°æ®: {start_date} åˆ° {end_date}")

        try:
            date_range = pd.date_range(start=start_date, end=end_date, freq='D')
            
            self.cds_client.retrieve(
                'reanalysis-era5-single-levels',
                {
                    'product_type': 'reanalysis',
                    'format': 'netcdf',
                    'variable': [
                        'mean_sea_level_pressure', '10m_u_component_of_wind',
                        '10m_v_component_of_wind', '2m_temperature',
                        'sea_surface_temperature', 'total_column_water_vapour'
                    ],
                    'year': sorted(list(set(date_range.year))),
                    'month': sorted(list(set(date_range.month))),
                    'day': sorted(list(set(date_range.day))),
                    'time': [
                        '00:00', '06:00', '12:00', '18:00'
                    ],
                    'area': area,
                },
                str(output_file)
            )

            print(f"âœ… ERA5æ•°æ®ä¸‹è½½å®Œæˆ: {output_file}")
            self._downloaded_files.append(str(output_file))
            return str(output_file)

        except Exception as e:
            print(f"âŒ ERA5æ•°æ®ä¸‹è½½å¤±è´¥: {e}")
            return None

    def download_era5_pressure_data(self, start_date, end_date, area=None, levels=("850","500","200")):
        """ä»CDSä¸‹è½½ERA5ç­‰å‹é¢æ•°æ®"""
        if area is None:
            lat_min = self.tracks_df['lat'].min() - 10
            lat_max = self.tracks_df['lat'].max() + 10
            lon_min = self.tracks_df['lon'].min() - 10
            lon_max = self.tracks_df['lon'].max() + 10
            area = [lat_max, lon_min, lat_min, lon_max]

        output_file = self.output_dir / f"era5_pressure_{start_date.replace('-', '')}_{end_date.replace('-', '')}.nc"

        if output_file.exists():
            print(f"ğŸ“ ERA5ç­‰å‹é¢æ•°æ®å·²å­˜åœ¨: {output_file}")
            self._downloaded_files.append(str(output_file))
            return str(output_file)

        print(f"ğŸ“¥ ä¸‹è½½ERA5ç­‰å‹é¢æ•°æ®: {start_date} åˆ° {end_date}")

        try:
            date_range = pd.date_range(start=start_date, end=end_date, freq='D')
            
            self.cds_client.retrieve(
                'reanalysis-era5-pressure-levels',
                {
                    'product_type': 'reanalysis',
                    'format': 'netcdf',
                    'variable': [
                        'u_component_of_wind', 'v_component_of_wind',
                        'geopotential', 'temperature', 'relative_humidity'
                    ],
                    'pressure_level': list(levels),
                    'year': sorted(list(set(date_range.year))),
                    'month': sorted(list(set(date_range.month))),
                    'day': sorted(list(set(date_range.day))),
                    'time': ['00:00', '06:00', '12:00', '18:00'],
                    'area': area,
                },
                str(output_file)
            )

            print(f"âœ… ERA5ç­‰å‹é¢æ•°æ®ä¸‹è½½å®Œæˆ: {output_file}")
            self._downloaded_files.append(str(output_file))
            return str(output_file)

        except Exception as e:
            print(f"âŒ ERA5ç­‰å‹é¢æ•°æ®ä¸‹è½½å¤±è´¥: {e}")
            return None

    def _parse_chunks_from_env(self):
        """ä»ç¯å¢ƒå˜é‡è§£æxarrayåˆ†å—è®¾ç½®"""
        chunks_str = os.environ.get(self.dask_chunks_env, "").strip()
        if not chunks_str:
            return None
        chunks = {}
        try:
            for part in chunks_str.split(','):
                k, v = part.split(':')
                chunks[k.strip()] = int(v.strip())
            return chunks
        except Exception:
            print(f"âš ï¸ æ— æ³•è§£æ {self.dask_chunks_env} ç¯å¢ƒå˜é‡çš„åˆ†å—è®¾ç½®: '{chunks_str}'")
            return None

    def load_era5_data(self, single_file, pressure_file=None):
        """åŠ è½½ERA5æ•°æ®æ–‡ä»¶"""
        try:
            chunks = self._parse_chunks_from_env()
            open_kwargs = {"chunks": chunks} if chunks else {}

            ds_single = xr.open_dataset(single_file, **open_kwargs)
            
            if pressure_file and Path(pressure_file).exists():
                ds_pressure = xr.open_dataset(pressure_file, **open_kwargs)
                self.ds = xr.merge([ds_single, ds_pressure])
            else:
                self.ds = ds_single
            
            print(f"ğŸ“Š ERA5æ•°æ®åŠ è½½å®Œæˆ: {dict(self.ds.dims)}")
            if 'latitude' in self.ds and 'longitude' in self.ds:
                print(f"ğŸŒ åæ ‡èŒƒå›´: lat {self.ds.latitude.min().values:.1f}Â°-{self.ds.latitude.max().values:.1f}Â°, "
                      f"lon {self.ds.longitude.min().values:.1f}Â°-{self.ds.longitude.max().values:.1f}Â°")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½ERA5æ•°æ®å¤±è´¥: {e}")
            return False

    def extract_environmental_systems(self, time_point, tc_lat, tc_lon):
        """æå–æŒ‡å®šæ—¶é—´ç‚¹çš„ç¯å¢ƒç³»ç»Ÿ"""
        systems = {}
        try:
            # *** FIX: ä½¿ç”¨ 'valid_time' åæ ‡ ***
            era5_time = pd.to_datetime(self.ds.valid_time.sel(valid_time=time_point, method='nearest').values)
            print(f"ğŸ” å¤„ç†æ—¶é—´ç‚¹: {time_point} (ä½¿ç”¨ERA5æ—¶é—´: {era5_time})")
            
            # *** FIX: ä½¿ç”¨ 'valid_time' åæ ‡è¿›è¡Œåˆ‡ç‰‡ ***
            ds_at_time = self.ds.sel(valid_time=time_point, method='nearest')

            systems['subtropical_high'] = self.extract_subtropical_high(ds_at_time, tc_lat, tc_lon)
            systems['ocean_heat'] = self.extract_ocean_heat(ds_at_time, tc_lat, tc_lon)
            systems['low_level_flow'] = self.extract_low_level_flow(ds_at_time, tc_lat, tc_lon)
            systems['atmospheric_stability'] = self.extract_atmospheric_stability(ds_at_time, tc_lat, tc_lon)
            systems['vertical_shear'] = self.extract_vertical_shear(ds_at_time, tc_lat, tc_lon)

        except Exception as e:
            print(f"âš ï¸ æå–ç¯å¢ƒç³»ç»Ÿå¤±è´¥: {e}")
            systems['error'] = str(e)
        return systems

    def extract_subtropical_high(self, ds_at_time, tc_lat, tc_lon):
        """æå–å‰¯çƒ­å¸¦é«˜å‹ç³»ç»Ÿ"""
        try:
            if 'z' in ds_at_time.data_vars and 'level' in ds_at_time.dims and 500 in ds_at_time.level.values:
                z500 = ds_at_time.sel(level=500)['z'] / 9.80665
                field = z500
                field_name = 'z500 (m)'
                high_by_height = True
            else:
                field = ds_at_time.msl / 100
                field_name = 'MSLP (hPa)'
                high_by_height = False

            # åœ¨å°é£å‘¨å›´æœç´¢é«˜å‹ç³»ç»Ÿ
            # æœç´¢èŒƒå›´æ‰©å¤§åˆ°æ•´ä¸ªæ•°æ®åŸŸï¼Œä»¥ç¡®ä¿æ•æ‰åˆ°è¿œè·ç¦»çš„é«˜å‹
            if high_by_height: # ä½åŠ¿é«˜åº¦æ‰¾æœ€å¤§å€¼
                max_val_idx = np.unravel_index(np.argmax(field.values), field.shape)
            else: # æµ·å¹³é¢æ°”å‹æ‰¾æœ€å¤§å€¼
                max_val_idx = np.unravel_index(np.argmax(field.values), field.shape)
            
            max_val = field.values[max_val_idx]
            high_lat = field.latitude.values[max_val_idx[0]]
            high_lon = field.longitude.values[max_val_idx[1]]

            bearing, distance = self._calculate_bearing_distance(tc_lat, tc_lon, high_lat, high_lon)

            if (not high_by_height) and max_val >= 1025: intensity_level = "å¼º"
            elif (not high_by_height) and max_val >= 1020: intensity_level = "ä¸­ç­‰"
            else: intensity_level = "å¼±"

            return {
                "system_type": "SubtropicalHigh", "position": {"lat": float(high_lat), "lon": float(high_lon)},
                "intensity": {"value": float(max_val), "unit": "m" if high_by_height else "hPa", "level": intensity_level, "field": field_name},
                "relative_to_tc": {"bearing_deg": float(bearing), "distance_km": float(distance), "direction": self._bearing_to_direction(bearing)},
                "description": f"{intensity_level}å‰¯çƒ­å¸¦é«˜å‹ç³»ç»Ÿä½äºå°é£{self._bearing_to_direction(bearing)}æ–¹å‘{int(distance)}kmå¤„",
                "influence": self._assess_high_influence(distance, max_val if not high_by_height else 1020, bearing)
            }
        except Exception as e:
            print(f"âš ï¸ æå–å‰¯çƒ­å¸¦é«˜å‹å¤±è´¥: {e}")
            return {"error": str(e)}

    def extract_ocean_heat(self, ds_at_time, tc_lat, tc_lon):
        """æå–æµ·æ´‹çƒ­å«é‡"""
        try:
            if 'sst' in ds_at_time.data_vars:
                temp_field = ds_at_time.sst
            elif 't2m' in ds_at_time.data_vars:
                temp_field = ds_at_time.t2m
            else:
                return {"error": "æ— æ¸©åº¦æ•°æ®"}
            
            if np.nanmean(temp_field.values) > 200:
                temp_field = temp_field - 273.15

            # åœ¨å°é£ä½ç½®æ’å€¼è·å–æ¸©åº¦
            point_temp = temp_field.interp(latitude=tc_lat, longitude=tc_lon, method="linear").values
            
            if point_temp >= 29: heat_level, energy_support = "æé«˜", "è¶…çº§æœ‰åˆ©"
            elif point_temp >= 28: heat_level, energy_support = "é«˜", "éå¸¸æœ‰åˆ©"
            elif point_temp >= 26.5: heat_level, energy_support = "ä¸­ç­‰", "åŸºæœ¬æœ‰åˆ©"
            else: heat_level, energy_support = "ä½", "èƒ½é‡ä¸è¶³"

            return {
                "system_type": "OceanHeatContent", "position": {"lat": tc_lat, "lon": tc_lon},
                "intensity": {"value": float(point_temp), "unit": "Â°C", "level": heat_level},
                "properties": {"energy_support": energy_support, "suitable_for_development": bool(point_temp >= 26.5)},
                "description": f"æµ·åŸŸæ¸©åº¦{point_temp:.1f}Â°Cï¼Œçƒ­å«é‡ç­‰çº§{heat_level}ï¼Œ{energy_support}å°é£å‘å±•"
            }
        except Exception as e:
            print(f"âš ï¸ æå–æµ·æ´‹çƒ­å«é‡å¤±è´¥: {e}")
            return {"error": str(e)}

    def extract_low_level_flow(self, ds_at_time, tc_lat, tc_lon):
        """æå–ä½å±‚é£åœº"""
        try:
            u10 = ds_at_time.u10.interp(latitude=tc_lat, longitude=tc_lon, method="linear").values
            v10 = ds_at_time.v10.interp(latitude=tc_lat, longitude=tc_lon, method="linear").values

            mean_speed = np.sqrt(u10**2 + v10**2)
            mean_direction = (np.degrees(np.arctan2(u10, v10)) + 360) % 360
            
            if mean_speed >= 15: wind_level = "å¼º"
            elif mean_speed >= 10: wind_level = "ä¸­ç­‰"
            else: wind_level = "å¼±"

            return {
                "system_type": "LowLevelFlow", "position": {"lat": tc_lat, "lon": tc_lon},
                "intensity": {"speed": float(mean_speed), "direction": float(mean_direction), "unit": "m/s", "level": wind_level, "vector": {"u": float(u10), "v": float(v10)}},
                "description": f"{wind_level}ä½å±‚é£åœºï¼Œé£é€Ÿ{mean_speed:.1f}m/sï¼Œé£å‘{mean_direction:.0f}Â°"
            }
        except Exception as e:
            print(f"âš ï¸ æå–ä½å±‚é£åœºå¤±è´¥: {e}")
            return {"error": str(e)}

    def extract_atmospheric_stability(self, ds_at_time, tc_lat, tc_lon):
        """æå–å¤§æ°”ç¨³å®šæ€§"""
        try:
            if 't2m' in ds_at_time.data_vars:
                t2m = ds_at_time.t2m
                if np.nanmean(t2m.values) > 200:
                    t2m = t2m - 273.15
                
                point_t2m = t2m.interp(latitude=tc_lat, longitude=tc_lon, method="linear").values
                stability = "ä¸­ç­‰" # ç®€åŒ–è¯„ä¼°

                return {
                    "system_type": "AtmosphericStability", "position": {"lat": tc_lat, "lon": tc_lon},
                    "intensity": {"surface_temp": float(point_t2m), "unit": "Â°C"},
                    "properties": {"stability_level": stability},
                    "description": f"è¿‘åœ°è¡¨æ¸©åº¦{point_t2m:.1f}Â°Cï¼Œå¤§æ°”ç¨³å®šæ€§{stability}"
                }
            else:
                return {"error": "æ— è¿‘åœ°è¡¨æ¸©åº¦æ•°æ®"}
        except Exception as e:
            print(f"âš ï¸ æå–å¤§æ°”ç¨³å®šæ€§å¤±è´¥: {e}")
            return {"error": str(e)}

    def extract_vertical_shear(self, ds_at_time, tc_lat, tc_lon):
        """æå–å‚ç›´é£åˆ‡å˜"""
        try:
            if 'u' in ds_at_time.data_vars and 'level' in ds_at_time.dims and 200 in ds_at_time.level.values and 850 in ds_at_time.level.values:
                u200 = ds_at_time.sel(level=200).u.interp(latitude=tc_lat, longitude=tc_lon, method="linear").values
                v200 = ds_at_time.sel(level=200).v.interp(latitude=tc_lat, longitude=tc_lon, method="linear").values
                u850 = ds_at_time.sel(level=850).u.interp(latitude=tc_lat, longitude=tc_lon, method="linear").values
                v850 = ds_at_time.sel(level=850).v.interp(latitude=tc_lat, longitude=tc_lon, method="linear").values

                du, dv = u200 - u850, v200 - v850
                shear = float(np.sqrt(du**2 + dv**2))
                shear_dir = float((np.degrees(np.arctan2(du, dv)) + 360) % 360)

                if shear >= 20: level = "å¼º"
                elif shear >= 12.5: level = "ä¸­ç­‰"
                else: level = "å¼±"

                return {
                    "system_type": "VerticalWindShear", "position": {"lat": tc_lat, "lon": tc_lon},
                    "intensity": {"magnitude": shear, "direction": shear_dir, "unit": "m/s", "level": level},
                    "description": f"å‚ç›´é£åˆ‡å˜ {shear:.1f} m/s ({level})ï¼Œæ–¹å‘ {shear_dir:.0f}Â°"
                }
            else:
                return {"error": "æœªåŠ è½½ç­‰å‹é¢é£åœºï¼Œæ— æ³•è®¡ç®—å‚ç›´é£åˆ‡å˜"}
        except Exception as e:
            print(f"âš ï¸ æå–å‚ç›´é£åˆ‡å˜å¤±è´¥: {e}")
            return {"error": str(e)}
            
    def _find_nearest_grid(self, lat, lon):
        lat_idx = np.abs(self.ds.latitude.values - lat).argmin()
        lon_idx = np.abs(self.ds.longitude.values - lon).argmin()
        return lat_idx, lon_idx

    def _calculate_bearing_distance(self, lat1, lon1, lat2, lon2):
        R = 6371.0
        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
        dlat, dlon = lat2 - lat1, lon2 - lon1
        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
        c = 2 * np.arcsin(np.sqrt(a))
        distance = R * c
        bearing = np.degrees(np.arctan2(np.sin(dlon) * np.cos(lat2), np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)))
        return (bearing + 360) % 360, distance

    def _bearing_to_direction(self, bearing):
        directions = [ (22.5, "åŒ—"), (67.5, "ä¸œåŒ—"), (112.5, "ä¸œ"), (157.5, "ä¸œå—"), (202.5, "å—"), (247.5, "è¥¿å—"), (292.5, "è¥¿"), (337.5, "è¥¿åŒ—"), (360, "åŒ—")]
        for limit, direction in directions:
            if bearing < limit: return direction
        return "åŒ—"

    def _assess_high_influence(self, distance, pressure, bearing):
        score = 0
        if distance < 500: score += 3
        elif distance < 1000: score += 2
        elif distance < 1500: score += 1
        if pressure >= 1025: score += 2
        elif pressure >= 1020: score += 1
        if 225 <= bearing <= 315: score += 1
        if score >= 4: return "å¼ºå½±å“"
        elif score >= 2: return "ä¸­ç­‰å½±å“"
        else: return "å¼±å½±å“"

    def process_all_tracks(self):
        """
        æŒ‰æœˆä¸‹è½½ã€å¤„ç†ã€ä¿å­˜å’Œæ¸…ç†æ•°æ®ï¼Œæ”¯æŒå¹¶è¡Œè®¡ç®—ã€‚
        """
        # æŒ‰å¹´æœˆåˆ†ç»„
        self.tracks_df['year_month'] = self.tracks_df['time'].dt.to_period('M')
        unique_months = sorted(self.tracks_df['year_month'].unique())
        print(f"ğŸ—“ï¸ æ‰¾åˆ° {len(unique_months)} ä¸ªéœ€è¦å¤„ç†çš„æœˆä»½: {[str(m) for m in unique_months]}")

        saved_files = []
        
        for month in unique_months:
            print(f"\n{'='*25} å¼€å§‹å¤„ç†æœˆä»½: {month} {'='*25}")
            month_tracks_df = self.tracks_df[self.tracks_df['year_month'] == month]
            start_date = month_tracks_df['time'].min().strftime('%Y-%m-%d')
            end_date = month_tracks_df['time'].max().strftime('%Y-%m-%d')
            print(f"ğŸ“… è¯¥æœˆæ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}ï¼Œå…± {len(month_tracks_df)} ä¸ªè·¯å¾„ç‚¹")
            
            single_file = self.download_era5_data(start_date, end_date)
            pressure_file = self.download_era5_pressure_data(start_date, end_date)
            
            if not single_file:
                print(f"âŒ æ— æ³•è·å– {month} çš„å•å±‚æ•°æ®ï¼Œè·³è¿‡æ­¤æœˆä»½")
                continue

            if not self.load_era5_data(single_file, pressure_file):
                print(f"âŒ æ— æ³•åŠ è½½ {month} çš„æ•°æ®ï¼Œè·³è¿‡æ­¤æœˆä»½")
                if self.cleanup_intermediate: self._cleanup_intermediate_files([single_file, pressure_file])
                continue

            # å®šä¹‰ç”¨äºå¹¶è¡Œå¤„ç†çš„å•ç‚¹å¤„ç†å‡½æ•°
            def _process_row(args):
                idx, track_point = args
                time_point = track_point['time']
                tc_lat, tc_lon = track_point['lat'], track_point['lon']
                print(f"ğŸ”„ å¤„ç†è·¯å¾„ç‚¹ {track_point['time_idx']+1}/{len(self.tracks_df)}: {time_point.strftime('%Y-%m-%d %H:%M')}")
                systems = self.extract_environmental_systems(time_point, tc_lat, tc_lon)
                return {
                    "time": time_point.isoformat(), "time_idx": int(track_point['time_idx']),
                    "tc_position": {"lat": tc_lat, "lon": tc_lon},
                    "tc_intensity": {"max_wind": track_point.get('max_wind_wmo', None), "min_pressure": track_point.get('min_pressure_wmo', None)},
                    "environmental_systems": systems
                }

            # å¹¶è¡Œæˆ–ä¸²è¡Œå¤„ç†å½“å‰æœˆä»½çš„è·¯å¾„ç‚¹
            iterable = list(month_tracks_df.iterrows())
            processed_this_month = []
            if self.max_workers is None or self.max_workers > 1:
                workers = self.max_workers or min(8, (os.cpu_count() or 1) + 4)
                print(f"âš™ï¸ ä½¿ç”¨ {workers} ä¸ªå·¥ä½œçº¿ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†...")
                with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as ex:
                    processed_this_month = list(ex.map(_process_row, iterable))
            else:
                print("âš™ï¸ æ­£åœ¨è¿›è¡Œä¸²è¡Œå¤„ç†...")
                processed_this_month = [_process_row(item) for item in iterable]

            # *** æ–°å¢ï¼šä¸ºå½“å‰æœˆä»½åˆ›å»ºå¹¶ä¿å­˜ç»“æœ ***
            if processed_this_month:
                monthly_results = {
                    "metadata": {
                        "extraction_time": datetime.now().isoformat(),
                        "tracks_file": str(self.tracks_file),
                        "total_points_in_month": len(processed_this_month),
                        "month_processed": str(month),
                        "data_source": "ERA5_reanalysis",
                        "processing_mode": "CDS_server_monthly_save"
                    },
                    "environmental_analysis": sorted(processed_this_month, key=lambda x: x['time_idx'])
                }
                
                monthly_output_file = self.output_dir / f"cds_environment_analysis_{month}.json"
                saved_path = self.save_results(monthly_results, output_file=monthly_output_file)
                if saved_path:
                    saved_files.append(saved_path)
            
            # æ¸…ç†å½“æœˆä¸‹è½½çš„æ–‡ä»¶
            if self.cleanup_intermediate:
                print(f"ğŸ§¹ æ­£åœ¨æ¸…ç† {month} çš„ä¸­é—´æ–‡ä»¶...")
                self._cleanup_intermediate_files([single_file, pressure_file])

        print(f"\nâœ… æ‰€æœ‰æœˆä»½å¤„ç†å®Œæ¯•ã€‚")
        return saved_files

    def save_results(self, results, output_file=None):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = self.output_dir / f"cds_environment_analysis_{timestamp}.json"
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {Path(output_file).stat().st_size / 1024:.1f} KB")
            return str(output_file)
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return None

    def _cleanup_intermediate_files(self, files_to_delete):
        """å…³é—­æ•°æ®é›†å¹¶åˆ é™¤æŒ‡å®šçš„ERA5ä¸´æ—¶æ–‡ä»¶ä»¥é‡Šæ”¾ç£ç›˜ç©ºé—´"""
        try:
            if hasattr(self, 'ds') and self.ds is not None:
                self.ds.close()
                self.ds = None
            gc.collect()

            removed_count = 0
            for f in files_to_delete:
                if f and Path(f).exists():
                    try:
                        Path(f).unlink()
                        removed_count += 1
                    except Exception as e:
                        print(f"âš ï¸ æ— æ³•åˆ é™¤æ–‡ä»¶ {f}: {e}")
            if removed_count > 0:
                print(f"ğŸ§¹ æˆåŠŸæ¸…ç† {removed_count} ä¸ªä¸­é—´æ•°æ®æ–‡ä»¶ã€‚")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ä¸­é—´æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    is_jupyter = 'ipykernel' in sys.modules

    if is_jupyter:
        print("ğŸŒ€ æ£€æµ‹åˆ° Jupyter ç¯å¢ƒï¼Œä½¿ç”¨é»˜è®¤å‚æ•°è¿è¡Œ")
        args = type('Args', (), {
            'tracks': 'western_pacific_typhoons_superfast.csv',
            'output': './cds_output',
            'max_points': None,
            'no_clean': False,
            'workers': None
        })()
    else:
        parser = argparse.ArgumentParser(description='CDSæœåŠ¡å™¨ç¯å¢ƒæ°”è±¡ç³»ç»Ÿæå–å™¨')
        parser.add_argument('--tracks', default='western_pacific_typhoons_superfast.csv', help='å°é£è·¯å¾„CSVæ–‡ä»¶è·¯å¾„')
        parser.add_argument('--output', default='./cds_output', help='è¾“å‡ºç›®å½•')
        parser.add_argument('--max-points', type=int, default=None, help='æœ€å¤§å¤„ç†è·¯å¾„ç‚¹æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
        parser.add_argument('--no-clean', action='store_true', help='ä¿ç•™ä¸­é—´ERA5æ•°æ®æ–‡ä»¶')
        parser.add_argument('--workers', type=int, default=None, help='å¹¶è¡Œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤è‡ªåŠ¨ï¼Œ1è¡¨ç¤ºç¦ç”¨å¹¶è¡Œï¼‰')
        args = parser.parse_args()

    print("ğŸŒ€ CDSç¯å¢ƒæ°”è±¡ç³»ç»Ÿæå–å™¨")
    print("=" * 50)
    print(f"ğŸ“ è·¯å¾„æ–‡ä»¶: {args.tracks}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output}")

    extractor = CDSEnvironmentExtractor(args.tracks, args.output, cleanup_intermediate=not args.no_clean, max_workers=args.workers)

    if args.max_points:
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: ä»…å¤„ç†å‰ {args.max_points} ä¸ªè·¯å¾„ç‚¹")
        extractor.tracks_df = extractor.tracks_df.head(args.max_points)

    # process_all_tracksç°åœ¨å¤„ç†æ‰€æœ‰äº‹æƒ…ï¼ŒåŒ…æ‹¬ä¿å­˜
    saved_file_list = extractor.process_all_tracks()

    if saved_file_list:
        print(f"\nâœ… å¤„ç†å®Œæˆï¼å…±ç”Ÿæˆ {len(saved_file_list)} ä¸ªæœˆåº¦ç»“æœæ–‡ä»¶:")
        for file_path in saved_file_list:
            print(f"  -> {file_path}")
    else:
        print("\nâŒ å¤„ç†å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆä»»ä½•ç»“æœæ–‡ä»¶ã€‚")
        sys.exit(1)


if __name__ == "__main__":
    main()