# Model source
MODEL_SOURCE=modelscope

# Model id on ModelScope
MODEL_ID=deepseek-ai/DeepSeek-R1-Distill-Llama-70B

# Optional: override the name returned by /v1/models and used in requests
SERVED_MODEL_NAME=

# Optional: set a local dir to download model files
# MODEL_DIR=deploy/vllm_local/models/DeepSeek-R1-Distill-Llama-70B

# Server
HOST=127.0.0.1
PORT=8000
API_KEY=

# vLLM settings
DTYPE=auto
# Optional engine args
MAX_MODEL_LEN=
TENSOR_PARALLEL_SIZE=
GPU_MEMORY_UTILIZATION=

# Extra args passed to vLLM (space-separated), e.g.:
# EXTRA_ARGS=--trust-remote-code --enforce-eager
EXTRA_ARGS=

# ModelScope cache (optional)
MS_CACHE_DIR=

# Logging
LOG_DIR=deploy/vllm_local/logs
PID_FILE=deploy/vllm_local/vllm_server.pid
